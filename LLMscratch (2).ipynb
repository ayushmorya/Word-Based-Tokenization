{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushmorya/Word-Based-Tokenization/blob/main/LLMscratch%20(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "2fff478f-9034-4dc6-8659-95d44240a78f",
      "cell_type": "markdown",
      "source": [
        "# Reading in a short story as text sample in Python."
      ],
      "metadata": {
        "id": "2fff478f-9034-4dc6-8659-95d44240a78f"
      }
    },
    {
      "id": "5f0d7c1d-4e63-4bfd-85e6-cb0e257ae3df",
      "cell_type": "markdown",
      "source": [
        "# Step 1: Creating Tokens"
      ],
      "metadata": {
        "id": "5f0d7c1d-4e63-4bfd-85e6-cb0e257ae3df"
      }
    },
    {
      "id": "2b329649-f338-485c-8399-a876fd71eadd",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e0f7e9; padding: 10px; border-left: 5px solid green;\">\n",
        "    <b>Note:</b> The print command prints the total number of characters followed by the first 100 characters of this file for illustration purposes.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "2b329649-f338-485c-8399-a876fd71eadd"
      }
    },
    {
      "id": "27c14564-d7cb-4a56-9738-c2c0bc800a46",
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"The total number of characters in the story:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27c14564-d7cb-4a56-9738-c2c0bc800a46",
        "outputId": "1a052f49-6ec7-4bdc-8d17-735b2434bf5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of characters in the story: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "id": "eada8891-13d6-4e83-a85e-a1efe56582b4",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e0f7e9; padding: 10px; border-left: 5px solid green;\">\n",
        "    <b>Note:</b> Our goal is to tokenize this 20479 character short story into individual words and special characters that we can then turn into embeddings for LLM training.\n",
        "</div>"
      ],
      "metadata": {
        "id": "eada8891-13d6-4e83-a85e-a1efe56582b4"
      }
    },
    {
      "id": "f54f11d2-fef5-437f-a981-6877d178e1b9",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#d0f0fd; padding: 10px; border-left: 5px solid #2196F3;\">\n",
        "    <b>Note:</b> Note that it's common to process millions of articles and hundreds of thousands of\n",
        "    books -- many gigabytes of text -- when working with LLMs. However, for educational\n",
        "    purposes, it's sufficient to work with smaller text samples like a single book to\n",
        "    illustrate the main ideas behind the text processing steps and to make it possible to\n",
        "    run it in reasonable time on consumer hardware.\n",
        "</div>"
      ],
      "metadata": {
        "id": "f54f11d2-fef5-437f-a981-6877d178e1b9"
      }
    },
    {
      "id": "2a718e62-fd3e-48c0-9064-d2829b03c41f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe0b2; padding: 10px; border-left: 5px solid orange;\">\n",
        "    <b>Note:</b> How can we best split this text to obtain a list of tokens? For this, we go on a small\n",
        "    excursion and use Python's regular expression library <code>re</code> for illustration purposes. (Note\n",
        "    that you don't have to learn or memorize any regular expression syntax since we will\n",
        "    transition to a pre-built tokenizer later in this chapter.)\n",
        "</div>"
      ],
      "metadata": {
        "id": "2a718e62-fd3e-48c0-9064-d2829b03c41f"
      }
    },
    {
      "id": "8ae62ae4-0ed5-4154-bd1f-1da033012338",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#d0f0fd; padding: 10px; border-left: 5px solid #2196F3;\">\n",
        "    <b>Info:</b> Using some simple example text, we can use the <code>re.split</code> command with the following\n",
        "    syntax to split a text on whitespace characters.\n",
        "</div>"
      ],
      "metadata": {
        "id": "8ae62ae4-0ed5-4154-bd1f-1da033012338"
      }
    },
    {
      "id": "900b6579-c277-49cf-872a-7cf40cb9e2ee",
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, World. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "900b6579-c277-49cf-872a-7cf40cb9e2ee",
        "outputId": "cd8a9d17-2847-4f93-d329-04e8637881c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'World.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "id": "2f33208e-35eb-4a70-b7a3-b5f3e1bc9751",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #9c27b0;\">\n",
        "    <b>Note:</b> The result is a list of individual words, whitespaces, and punctuation characters.\n",
        "</div>"
      ],
      "metadata": {
        "id": "2f33208e-35eb-4a70-b7a3-b5f3e1bc9751"
      }
    },
    {
      "id": "0dd3c4cb-5702-447c-91e9-5a4816cbbc5c",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff9c4; padding: 10px; border-left: 5px solid #fbc02d;\">\n",
        "    <b>Note:</b> Let's modify the regular expression splits on whitespaces (<code>\\s</code>) and commas, and periods (<code>[,.]</code>).\n",
        "</div>"
      ],
      "metadata": {
        "id": "0dd3c4cb-5702-447c-91e9-5a4816cbbc5c"
      }
    },
    {
      "id": "d3eeb254-c756-4722-94a4-cc3f9785c9a8",
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3eeb254-c756-4722-94a4-cc3f9785c9a8",
        "outputId": "b4c7d0d4-4ae3-40c1-8f78-bd25e94fa9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'World', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "id": "7b61ade4-463f-44bc-9976-aec7d6f65090",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e0f7e9; padding: 10px; border-left: 5px solid green;\">\n",
        "    <b>Note:</b> We can see that the words and punctuation characters are now separate list entries just as we wanted (<code>[,.]</code>).\n",
        "</div>"
      ],
      "metadata": {
        "id": "7b61ade4-463f-44bc-9976-aec7d6f65090"
      }
    },
    {
      "id": "58dd193b-f4e3-4eb4-88b0-8e45f7dd23c2",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #9c27b0;\">\n",
        "    <b>Note:</b> A small remaining issue is that the list still includes whitespace characters. Optionally, we\n",
        "    can remove these redundant characters safely as follows:\n",
        "</div>"
      ],
      "metadata": {
        "id": "58dd193b-f4e3-4eb4-88b0-8e45f7dd23c2"
      }
    },
    {
      "id": "d0ef262e-37e5-4725-abe9-9c24dfb99c01",
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0ef262e-37e5-4725-abe9-9c24dfb99c01",
        "outputId": "ba2c7b5c-88e9-48ec-96cc-b01819488dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'World', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "id": "6dc5c372-8160-4524-90dc-4eab791c1f14",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe0b2; padding: 10px; border-left: 5px solid orange;\">\n",
        "    <b>REMOVING WHITESPACES OR NOT</b><br><br>\n",
        "    When developing a simple tokenizer, whether we should encode whitespaces as\n",
        "    separate characters or just remove them depends on our application and its\n",
        "    requirements. Removing whitespaces reduces the memory and computing\n",
        "    requirements. However, keeping whitespaces can be useful if we train models that\n",
        "    are sensitive to the exact structure of the text (for example, Python code, which is\n",
        "    sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
        "    and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
        "    that includes whitespaces.\n",
        "</div>"
      ],
      "metadata": {
        "id": "6dc5c372-8160-4524-90dc-4eab791c1f14"
      }
    },
    {
      "id": "a98f676f-85ea-4db8-ad1e-5ea50831e2a9",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#bbdefb; padding: 10px; border-left: 5px solid #2196f3;\">\n",
        "    <b>Note:</b> The tokenization scheme we devised above works well on the simple sample text. Let's\n",
        "    modify it a bit further so that it can also handle other types of punctuation, such as\n",
        "    question marks, quotation marks, and the double-dashes we have seen earlier in the first\n",
        "    100 characters of Edith Wharton's short story, along with additional special characters.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "a98f676f-85ea-4db8-ad1e-5ea50831e2a9"
      }
    },
    {
      "id": "4f9b69d3-dcf0-47f5-ac63-2f4e8bae7789",
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f9b69d3-dcf0-47f5-ac63-2f4e8bae7789",
        "outputId": "f0b9e15a-84aa-4562-a341-4467bc93848e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "id": "d4259f78-e21b-409c-acf8-1138e7fa0bf8",
      "cell_type": "code",
      "source": [
        "# Strip whitespace from each item and then filter our any empty string\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4259f78-e21b-409c-acf8-1138e7fa0bf8",
        "outputId": "9c11e1b7-64d7-420c-d394-04fe83831fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "id": "f592b777-00e8-4609-980b-91a555d2d006",
      "cell_type": "code",
      "source": [
        "text = \"And I, Am...Iron Ma!\"\n",
        "result = re.split(r'(\\.\\.\\.|[,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f592b777-00e8-4609-980b-91a555d2d006",
        "outputId": "3ac02d58-3f29-418d-f348-42f53e9a826f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['And', 'I', ',', 'Am', '...', 'Iron', 'Ma', '!']\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "id": "1a4b0a9e-8c73-4a45-ac8d-4314e9e13f9f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e8f5e9; padding: 10px; border-left: 5px solid #43a047;\">\n",
        "    <b>Note:</b> Now that we got a basic tokenizer, let's apply it to the story.\n",
        "</div>"
      ],
      "metadata": {
        "id": "1a4b0a9e-8c73-4a45-ac8d-4314e9e13f9f"
      }
    },
    {
      "id": "8d34e68c-197d-48fd-bf72-32d957e0fa66",
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'(\\.\\.\\.|[,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d34e68c-197d-48fd-bf72-32d957e0fa66",
        "outputId": "401e3e8a-3302-4f7c-93bf-8b940dac9f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "id": "a83c4811-2fbb-4588-9983-d33fa6c7e003",
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a83c4811-2fbb-4588-9983-d33fa6c7e003",
        "outputId": "701353b1-59bf-4f7a-eb17-ef59bb0b7460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "id": "cd9aa5da-b08a-407d-9027-b2d33798f619",
      "cell_type": "markdown",
      "source": [
        "# Step: 2 Creating Token IDs"
      ],
      "metadata": {
        "id": "cd9aa5da-b08a-407d-9027-b2d33798f619"
      }
    },
    {
      "id": "023b4f05-63a1-4445-9617-8dd86c513f31",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #8e24aa;\">\n",
        "    <b>Note:</b> In the previous section, we tokenized Edith Wharton's short story and assigned it to a\n",
        "    Python variable called <code>preprocessed</code>. Let's now create a list of all unique tokens and sort\n",
        "    them alphabetically to determine the vocabulary size.\n",
        "</div>"
      ],
      "metadata": {
        "id": "023b4f05-63a1-4445-9617-8dd86c513f31"
      }
    },
    {
      "id": "87a914b6-e296-4fd7-a4bc-54dca7cc6175",
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87a914b6-e296-4fd7-a4bc-54dca7cc6175",
        "outputId": "121b26fe-a5e3-4a84-ed05-98f1502a4053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "id": "f87c2308-3363-4295-b3ad-36618216fd45",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff3e0; padding: 10px; border-left: 5px solid #fb8c00;\">\n",
        "    <b>Note:</b> After determining that the vocabulary size is 1,130 via the above code, we create the\n",
        "    vocabulary and print its first 51 entries for illustration purposes.\n",
        "</div>"
      ],
      "metadata": {
        "id": "f87c2308-3363-4295-b3ad-36618216fd45"
      }
    },
    {
      "id": "9d7bf329-8f64-4aac-be16-446e1cb12dbe",
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "trusted": true,
        "id": "9d7bf329-8f64-4aac-be16-446e1cb12dbe"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "id": "a2e99fe4-2aeb-4f6e-a2b7-f34f59566afd",
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i>=50:\n",
        "        break"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e99fe4-2aeb-4f6e-a2b7-f34f59566afd",
        "outputId": "dd893b58-c7f1-4ec4-aaf6-9278339e2bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "id": "77f4ddba-48da-411c-8ebf-9d074b598002",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff9c4; padding: 10px; border-left: 5px solid #fbc02d;\">\n",
        "    <b>Note:</b> As we can see, based on the output above, the dictionary contains individual tokens\n",
        "    associated with unique integer labels.\n",
        "</div>"
      ],
      "metadata": {
        "id": "77f4ddba-48da-411c-8ebf-9d074b598002"
      }
    },
    {
      "id": "57496f2f-51f6-4285-ac40-1b615069e043",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#d0f0c0; padding: 10px; border-left: 5px solid #388e3c;\">\n",
        "    <b>Note:</b> Later in this book, when we want to convert the outputs of an LLM from numbers back into\n",
        "    text, we also need a way to turn token IDs into text. <br><br>\n",
        "    For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
        "</div>"
      ],
      "metadata": {
        "id": "57496f2f-51f6-4285-ac40-1b615069e043"
      }
    },
    {
      "id": "e389335d-f299-4479-b2b9-836a46a2da9e",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e3f2fd; padding: 10px; border-left: 5px solid #1e88e5;\">\n",
        "    <b>Note:</b> Let's implement a complete tokenizer class in Python. <br><br>\n",
        "    The class will have an <code>encode</code> method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. <br><br>\n",
        "    In addition, we implement a <code>decode</code> method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.\n",
        "</div>"
      ],
      "metadata": {
        "id": "e389335d-f299-4479-b2b9-836a46a2da9e"
      }
    },
    {
      "id": "1b9ae355-2a72-4299-9771-cc6a4a34d05e",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #8e24aa;\">\n",
        "    <b>Steps:</b><br>\n",
        "    Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods<br>\n",
        "    Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens<br>\n",
        "    Step 3: Process input text into token IDs<br>\n",
        "    Step 4: Convert token IDs back into text<br>\n",
        "    Step 5: Replace spaces before the specified punctuation\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "1b9ae355-2a72-4299-9771-cc6a4a34d05e"
      }
    },
    {
      "id": "56a3817d-18d2-498a-a25e-7c6fa62d33e3",
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'(\\.\\.\\.|--|[.,;:?!\"()\\']|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        #Replace spaces vefore the specified punctuaions\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "trusted": true,
        "id": "56a3817d-18d2-498a-a25e-7c6fa62d33e3"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "id": "3d45d6f0-d29d-409c-881b-6ac4cf31abf0",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff3e0; padding: 10px; border-left: 5px solid #fb8c00;\">\n",
        "    Let's instantiate a new tokenizer object from the <code>SimpleTokenizerV1</code> class and tokenize a\n",
        "    passage from Edith Wharton's short story to try it out in practice:\n",
        "</div>"
      ],
      "metadata": {
        "id": "3d45d6f0-d29d-409c-881b-6ac4cf31abf0"
      }
    },
    {
      "id": "6c17438a-74f8-4387-ac08-033c928911e5",
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c17438a-74f8-4387-ac08-033c928911e5",
        "outputId": "ed1d6293-b864-439c-b03a-26bd618ce6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "id": "21d2ad40-2cb8-4bb4-8c1c-bac97522b452",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e8f5e9; padding: 10px; border-left: 5px solid #43a047;\">\n",
        "    <b>Note:</b> The code above prints the following token IDs:<br>\n",
        "    Next, let's see if we can turn these token IDs back into text using the <code>decode</code> method.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "21d2ad40-2cb8-4bb4-8c1c-bac97522b452"
      }
    },
    {
      "id": "9771cacb-6ac6-415e-afab-20fa304606e3",
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9771cacb-6ac6-415e-afab-20fa304606e3",
        "outputId": "cc3968e2-1e6b-4fde-824d-803cfa8782b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "execution_count": 18
    },
    {
      "id": "c87d56f1-a927-4f9f-9587-037bec7b2020",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff9c4; padding: 10px; border-left: 5px solid #fbc02d;\">\n",
        "    <b>Note:</b> Based on the output above, we can see that the <code>decode</code> method successfully converted the\n",
        "    token IDs back into the original text.\n",
        "</div>"
      ],
      "metadata": {
        "id": "c87d56f1-a927-4f9f-9587-037bec7b2020"
      }
    },
    {
      "id": "5c5dc0d0-31d7-4a10-b728-0bafa6609134",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e3f2fd; padding: 10px; border-left: 5px solid #2196f3;\">\n",
        "    <b>Note:</b> So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing\n",
        "    text based on a snippet from the training set. <br><br>\n",
        "    Let's now apply it to a new text sample that is not contained in the training set:\n",
        "</div>"
      ],
      "metadata": {
        "id": "5c5dc0d0-31d7-4a10-b728-0bafa6609134"
      }
    },
    {
      "id": "b906b02d-9f57-4aaa-885a-eb4126ef9222",
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "b906b02d-9f57-4aaa-885a-eb4126ef9222",
        "outputId": "1c730cae-83e4-42e4-afa7-b9d31c821f2a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1763555282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3743522749.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(\\.\\.\\.|--|[.,;:?!\"()\\']|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3743522749.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(\\.\\.\\.|--|[.,;:?!\"()\\']|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "id": "3655337b-22d3-468b-b8da-0008322f2070",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #9c27b0;\">\n",
        "    <b>Note:</b> The problem is that the word \"Hello\" was not used in the <i>The Verdict</i> short story. <br><br>\n",
        "    Hence, it is not contained in the vocabulary. <br><br>\n",
        "    This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs.\n",
        "</div>"
      ],
      "metadata": {
        "id": "3655337b-22d3-468b-b8da-0008322f2070"
      }
    },
    {
      "id": "4c4c3e80-08e5-4a67-9e73-36151d019186",
      "cell_type": "markdown",
      "source": [
        "# ADDING SPECIAL CONTEXT TOKENS\n",
        "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
        "from the training set.\n",
        "\n",
        "In this section, we will modify this tokenizer to handle unknown\n",
        "words.\n",
        "\n",
        "\n",
        "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
        "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
        "<|endoftext|>"
      ],
      "metadata": {
        "id": "4c4c3e80-08e5-4a67-9e73-36151d019186"
      }
    },
    {
      "id": "47f975f2-796e-4d6a-b105-ec5c4d2b6d49",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff3e0; padding: 10px; border-left: 5px solid #fb8c00;\">\n",
        "    <b>Note:</b> We can modify the tokenizer to use an <code>&lt;|unk|&gt;</code> token if it encounters a word that is not part of the vocabulary. <br><br>\n",
        "    Furthermore, we add a token between unrelated texts. <br><br>\n",
        "    For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source.\n",
        "</div>"
      ],
      "metadata": {
        "id": "47f975f2-796e-4d6a-b105-ec5c4d2b6d49"
      }
    },
    {
      "id": "237e176f-807a-4aad-8e14-1277f95dfc1e",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fce4ec; padding: 10px; border-left: 5px solid #f06292;\">\n",
        "    <b>Note:</b> Let's now modify the vocabulary to include these two special tokens, <code>&lt;unk&gt;</code> and <code>&lt;|endoftext|&gt;</code>, by adding these to the list of all unique words that we created in the previous section.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "237e176f-807a-4aad-8e14-1277f95dfc1e"
      }
    },
    {
      "id": "a6cc522d-a0fd-44d8-ad64-0108fa7db4f8",
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "trusted": true,
        "id": "a6cc522d-a0fd-44d8-ad64-0108fa7db4f8"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "id": "5edd3242-81d1-420f-916f-e2c6f4bba02e",
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5edd3242-81d1-420f-916f-e2c6f4bba02e",
        "outputId": "8e85a427-1de2-4fdd-d5e5-013a627ee975"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "execution_count": 21
    },
    {
      "id": "67db441b-607d-4e3f-9817-35f14d464546",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff9c4; padding: 10px; border-left: 5px solid #fbc02d;\">\n",
        "    <b>Note:</b> Based on the output of the print statement above, the new vocabulary size is 1132 (the vocabulary size in the previous section was 1130).\n",
        "</div>"
      ],
      "metadata": {
        "id": "67db441b-607d-4e3f-9817-35f14d464546"
      }
    },
    {
      "id": "87c6cc36-9e20-4dc2-9c64-f148c780946f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eeeeee; padding: 10px; border-left: 5px solid #9e9e9e;\">\n",
        "    <b>Note:</b> As an additional quick check, let's print the last 5 entries of the updated vocabulary.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "87c6cc36-9e20-4dc2-9c64-f148c780946f"
      }
    },
    {
      "id": "91c14e87-f588-4f43-af85-92126506ca7f",
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c14e87-f588-4f43-af85-92126506ca7f",
        "outputId": "def7e074-f893-4dbb-eb38-c7f5cb536ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "id": "f829fe62-7a73-4287-8a2a-94f9bab97de0",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e8f5e9; padding: 10px; border-left: 5px solid #43a047;\">\n",
        "    <b>Note:</b> A simple text tokenizer that handles unknown words.\n",
        "</div>"
      ],
      "metadata": {
        "id": "f829fe62-7a73-4287-8a2a-94f9bab97de0"
      }
    },
    {
      "id": "b7eeec1f-e493-4742-9c3b-e2c0a2bca947",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e3f2fd; padding: 10px; border-left: 5px solid #1e88e5;\">\n",
        "    <b>Info:</b>\n",
        "    <ul style=\"margin: 0; padding-left: 20px;\">\n",
        "        <li>Step 1: Replace unknown words by <code>&lt;|unk|&gt;</code> tokens</li>\n",
        "        <li>Step 2: Replace spaces before the specified punctuations</li>\n",
        "    </ul>\n",
        "</div>"
      ],
      "metadata": {
        "id": "b7eeec1f-e493-4742-9c3b-e2c0a2bca947"
      }
    },
    {
      "id": "5fa74605-b59e-47d1-8e21-6a2509fbb805",
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'(\\.\\.\\.|--|[.,;:?!\"()\\']|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        #Replace spaces vefore the specified punctuaions\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "trusted": true,
        "id": "5fa74605-b59e-47d1-8e21-6a2509fbb805"
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "id": "98405bba-834e-4b11-bd1d-7a5477898a6c",
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98405bba-834e-4b11-bd1d-7a5477898a6c",
        "outputId": "90573539-26ad-4037-ba65-67c49bb6e50f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "id": "9783f8ce-518a-4831-89fa-b33d9e00030a",
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9783f8ce-518a-4831-89fa-b33d9e00030a",
        "outputId": "bc5ca383-9912-45f0-e0a3-bccfe6639a25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "execution_count": 25
    },
    {
      "id": "18e6fedc-408b-4260-993f-79498acea426",
      "cell_type": "code",
      "source": [
        "tokenizer.decode (tokenizer.encode(text))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "18e6fedc-408b-4260-993f-79498acea426",
        "outputId": "ce5dff7c-cb86-4e46-e435-e15de4c5b25d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "execution_count": 29
    },
    {
      "id": "95299169-f284-417f-ae6d-df22f078cfd4",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#d0f0c0; padding: 10px; border-left: 5px solid #4caf50;\">\n",
        "    <b>Info:</b> Based on comparing the de-tokenized text above with the original input text, we know that\n",
        "    the training dataset, Edith Wharton's short story <i>The Verdict</i>, did not contain the words\n",
        "    <code>\"Hello\"</code> and <code>\"palace\"</code>.\n",
        "</div>"
      ],
      "metadata": {
        "id": "95299169-f284-417f-ae6d-df22f078cfd4"
      }
    },
    {
      "id": "8c7a47ef-3ae6-42be-93f2-a123fcdd539f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e0f7fa; padding: 10px; border-left: 5px solid #0097a7;\">\n",
        "  <b>Info:</b> So far, we have discussed tokenization as an essential step in processing text as input to\n",
        "  LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
        "  as the following:\n",
        "  <ul>\n",
        "    <li><b>[BOS]</b> (beginning of sequence): This token marks the start of a text. It\n",
        "    signifies to the LLM where a piece of content begins.</li>\n",
        "    <li><b>[EOS]</b> (end of sequence): This token is positioned at the end of a text,\n",
        "    and is especially useful when concatenating multiple unrelated texts,\n",
        "    similar to <code>&lt;|endoftext|&gt;</code>. For instance, when combining two different\n",
        "    Wikipedia articles or books, the [EOS] token indicates where one article\n",
        "    ends and the next one begins.</li>\n",
        "    <li><b>[PAD]</b> (padding): When training LLMs with batch sizes larger than one,\n",
        "    the batch might contain texts of varying lengths. To ensure all texts have\n",
        "    the same length, the shorter texts are extended or \"padded\" using the\n",
        "    [PAD] token, up to the length of the longest text in the batch.</li>\n",
        "  </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "8c7a47ef-3ae6-42be-93f2-a123fcdd539f"
      }
    },
    {
      "id": "829b9e91-f04a-4840-bf25-e7d780625f4f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #9c27b0;\">\n",
        "  <b>Note:</b> Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
        "  above but only uses an <code>&lt;|endoftext|&gt;</code> token for simplicity.\n",
        "</div>"
      ],
      "metadata": {
        "id": "829b9e91-f04a-4840-bf25-e7d780625f4f"
      }
    },
    {
      "id": "88c9df3b-b7c3-48da-9dc2-02f9a059c4ec",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff3e0; padding: 10px; border-left: 5px solid #fb8c00;\">\n",
        "  <b>Info:</b> The tokenizer used for GPT models also doesn't use an <code>&lt;|unk|&gt;</code> token for out-of-vocabulary words.\n",
        "  Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "88c9df3b-b7c3-48da-9dc2-02f9a059c4ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KBldn7N5eIr",
        "outputId": "55b1a244-aff9-4b3a-f570-32b9c1f5e24c"
      },
      "id": "4KBldn7N5eIr",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "B6cIzeGq55F1"
      },
      "id": "B6cIzeGq55F1",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit of terraces\"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JqSjIwC5_3a",
        "outputId": "84b53956-8aa7-46c0-b9ef-0fff0536c1f5"
      },
      "id": "4JqSjIwC5_3a",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 286, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AVX43CG92vl"
      },
      "id": "6AVX43CG92vl",
      "execution_count": null,
      "outputs": []
    }
  ]
}