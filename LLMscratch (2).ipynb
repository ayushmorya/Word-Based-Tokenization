{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "2fff478f-9034-4dc6-8659-95d44240a78f",
      "cell_type": "markdown",
      "source": [
        "# Reading in a short story as text sample in Python."
      ],
      "metadata": {
        "id": "2fff478f-9034-4dc6-8659-95d44240a78f"
      }
    },
    {
      "id": "5f0d7c1d-4e63-4bfd-85e6-cb0e257ae3df",
      "cell_type": "markdown",
      "source": [
        "# Step 1: Creating Tokens"
      ],
      "metadata": {
        "id": "5f0d7c1d-4e63-4bfd-85e6-cb0e257ae3df"
      }
    },
    {
      "id": "2b329649-f338-485c-8399-a876fd71eadd",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e0f7e9; padding: 10px; border-left: 5px solid green;\">\n",
        "    <b>Note:</b> The print command prints the total number of characters followed by the first 100 characters of this file for illustration purposes.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "2b329649-f338-485c-8399-a876fd71eadd"
      }
    },
    {
      "id": "27c14564-d7cb-4a56-9738-c2c0bc800a46",
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"The total number of characters in the story:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "27c14564-d7cb-4a56-9738-c2c0bc800a46",
        "outputId": "1ded8705-2197-4e41-8764-e9c319354d98"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'the-verdict.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2241892684.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the-verdict.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The total number of characters in the story:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the-verdict.txt'"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "id": "eada8891-13d6-4e83-a85e-a1efe56582b4",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e0f7e9; padding: 10px; border-left: 5px solid green;\">\n",
        "    <b>Note:</b> Our goal is to tokenize this 20479 character short story into individual words and special characters that we can then turn into embeddings for LLM training.\n",
        "</div>"
      ],
      "metadata": {
        "id": "eada8891-13d6-4e83-a85e-a1efe56582b4"
      }
    },
    {
      "id": "f54f11d2-fef5-437f-a981-6877d178e1b9",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#d0f0fd; padding: 10px; border-left: 5px solid #2196F3;\">\n",
        "    <b>Note:</b> Note that it's common to process millions of articles and hundreds of thousands of\n",
        "    books -- many gigabytes of text -- when working with LLMs. However, for educational\n",
        "    purposes, it's sufficient to work with smaller text samples like a single book to\n",
        "    illustrate the main ideas behind the text processing steps and to make it possible to\n",
        "    run it in reasonable time on consumer hardware.\n",
        "</div>"
      ],
      "metadata": {
        "id": "f54f11d2-fef5-437f-a981-6877d178e1b9"
      }
    },
    {
      "id": "2a718e62-fd3e-48c0-9064-d2829b03c41f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe0b2; padding: 10px; border-left: 5px solid orange;\">\n",
        "    <b>Note:</b> How can we best split this text to obtain a list of tokens? For this, we go on a small\n",
        "    excursion and use Python's regular expression library <code>re</code> for illustration purposes. (Note\n",
        "    that you don't have to learn or memorize any regular expression syntax since we will\n",
        "    transition to a pre-built tokenizer later in this chapter.)\n",
        "</div>"
      ],
      "metadata": {
        "id": "2a718e62-fd3e-48c0-9064-d2829b03c41f"
      }
    },
    {
      "id": "8ae62ae4-0ed5-4154-bd1f-1da033012338",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#d0f0fd; padding: 10px; border-left: 5px solid #2196F3;\">\n",
        "    <b>Info:</b> Using some simple example text, we can use the <code>re.split</code> command with the following\n",
        "    syntax to split a text on whitespace characters.\n",
        "</div>"
      ],
      "metadata": {
        "id": "8ae62ae4-0ed5-4154-bd1f-1da033012338"
      }
    },
    {
      "id": "900b6579-c277-49cf-872a-7cf40cb9e2ee",
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, World. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "900b6579-c277-49cf-872a-7cf40cb9e2ee",
        "outputId": "5a0a5b31-d14b-44b1-b1a3-d215315641df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'World.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "2f33208e-35eb-4a70-b7a3-b5f3e1bc9751",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #9c27b0;\">\n",
        "    <b>Note:</b> The result is a list of individual words, whitespaces, and punctuation characters.\n",
        "</div>"
      ],
      "metadata": {
        "id": "2f33208e-35eb-4a70-b7a3-b5f3e1bc9751"
      }
    },
    {
      "id": "0dd3c4cb-5702-447c-91e9-5a4816cbbc5c",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff9c4; padding: 10px; border-left: 5px solid #fbc02d;\">\n",
        "    <b>Note:</b> Let's modify the regular expression splits on whitespaces (<code>\\s</code>) and commas, and periods (<code>[,.]</code>).\n",
        "</div>"
      ],
      "metadata": {
        "id": "0dd3c4cb-5702-447c-91e9-5a4816cbbc5c"
      }
    },
    {
      "id": "d3eeb254-c756-4722-94a4-cc3f9785c9a8",
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3eeb254-c756-4722-94a4-cc3f9785c9a8",
        "outputId": "1a20cf52-73e9-4afa-9dc5-fa4ce6d67fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'World', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "7b61ade4-463f-44bc-9976-aec7d6f65090",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e0f7e9; padding: 10px; border-left: 5px solid green;\">\n",
        "    <b>Note:</b> We can see that the words and punctuation characters are now separate list entries just as we wanted (<code>[,.]</code>).\n",
        "</div>"
      ],
      "metadata": {
        "id": "7b61ade4-463f-44bc-9976-aec7d6f65090"
      }
    },
    {
      "id": "58dd193b-f4e3-4eb4-88b0-8e45f7dd23c2",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #9c27b0;\">\n",
        "    <b>Note:</b> A small remaining issue is that the list still includes whitespace characters. Optionally, we\n",
        "    can remove these redundant characters safely as follows:\n",
        "</div>"
      ],
      "metadata": {
        "id": "58dd193b-f4e3-4eb4-88b0-8e45f7dd23c2"
      }
    },
    {
      "id": "d0ef262e-37e5-4725-abe9-9c24dfb99c01",
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0ef262e-37e5-4725-abe9-9c24dfb99c01",
        "outputId": "152234b4-031d-463e-970f-e69d8acab707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'World', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "6dc5c372-8160-4524-90dc-4eab791c1f14",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe0b2; padding: 10px; border-left: 5px solid orange;\">\n",
        "    <b>REMOVING WHITESPACES OR NOT</b><br><br>\n",
        "    When developing a simple tokenizer, whether we should encode whitespaces as\n",
        "    separate characters or just remove them depends on our application and its\n",
        "    requirements. Removing whitespaces reduces the memory and computing\n",
        "    requirements. However, keeping whitespaces can be useful if we train models that\n",
        "    are sensitive to the exact structure of the text (for example, Python code, which is\n",
        "    sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
        "    and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
        "    that includes whitespaces.\n",
        "</div>"
      ],
      "metadata": {
        "id": "6dc5c372-8160-4524-90dc-4eab791c1f14"
      }
    },
    {
      "id": "a98f676f-85ea-4db8-ad1e-5ea50831e2a9",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#bbdefb; padding: 10px; border-left: 5px solid #2196f3;\">\n",
        "    <b>Note:</b> The tokenization scheme we devised above works well on the simple sample text. Let's\n",
        "    modify it a bit further so that it can also handle other types of punctuation, such as\n",
        "    question marks, quotation marks, and the double-dashes we have seen earlier in the first\n",
        "    100 characters of Edith Wharton's short story, along with additional special characters.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "a98f676f-85ea-4db8-ad1e-5ea50831e2a9"
      }
    },
    {
      "id": "4f9b69d3-dcf0-47f5-ac63-2f4e8bae7789",
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f9b69d3-dcf0-47f5-ac63-2f4e8bae7789",
        "outputId": "bddb9ecd-04c4-41cd-9f58-acc306724799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "d4259f78-e21b-409c-acf8-1138e7fa0bf8",
      "cell_type": "code",
      "source": [
        "# Strip whitespace from each item and then filter our any empty string\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4259f78-e21b-409c-acf8-1138e7fa0bf8",
        "outputId": "113510ed-317a-4bc8-9787-a025b04a8d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "f592b777-00e8-4609-980b-91a555d2d006",
      "cell_type": "code",
      "source": [
        "text = \"And I, Am...Iron Ma!\"\n",
        "result = re.split(r'(\\.\\.\\.|[,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f592b777-00e8-4609-980b-91a555d2d006",
        "outputId": "494f872a-03f2-4b61-a861-67a52af50edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['And', 'I', ',', 'Am', '...', 'Iron', 'Ma', '!']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "1a4b0a9e-8c73-4a45-ac8d-4314e9e13f9f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e8f5e9; padding: 10px; border-left: 5px solid #43a047;\">\n",
        "    <b>Note:</b> Now that we got a basic tokenizer, let's apply it to the story.\n",
        "</div>"
      ],
      "metadata": {
        "id": "1a4b0a9e-8c73-4a45-ac8d-4314e9e13f9f"
      }
    },
    {
      "id": "8d34e68c-197d-48fd-bf72-32d957e0fa66",
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'(\\.\\.\\.|[,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d34e68c-197d-48fd-bf72-32d957e0fa66",
        "outputId": "19b7a49b-c4e8-4c3e-c2b8-ae91a77be4c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "a83c4811-2fbb-4588-9983-d33fa6c7e003",
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a83c4811-2fbb-4588-9983-d33fa6c7e003",
        "outputId": "d506bbaf-3e10-493d-c644-e8cc3bdcab7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "cd9aa5da-b08a-407d-9027-b2d33798f619",
      "cell_type": "markdown",
      "source": [
        "# Step: 2 Creating Token IDs"
      ],
      "metadata": {
        "id": "cd9aa5da-b08a-407d-9027-b2d33798f619"
      }
    },
    {
      "id": "023b4f05-63a1-4445-9617-8dd86c513f31",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #8e24aa;\">\n",
        "    <b>Note:</b> In the previous section, we tokenized Edith Wharton's short story and assigned it to a\n",
        "    Python variable called <code>preprocessed</code>. Let's now create a list of all unique tokens and sort\n",
        "    them alphabetically to determine the vocabulary size.\n",
        "</div>"
      ],
      "metadata": {
        "id": "023b4f05-63a1-4445-9617-8dd86c513f31"
      }
    },
    {
      "id": "87a914b6-e296-4fd7-a4bc-54dca7cc6175",
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87a914b6-e296-4fd7-a4bc-54dca7cc6175",
        "outputId": "98be2d2b-2e5a-470e-ff4b-b47438c6b97d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "f87c2308-3363-4295-b3ad-36618216fd45",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff3e0; padding: 10px; border-left: 5px solid #fb8c00;\">\n",
        "    <b>Note:</b> After determining that the vocabulary size is 1,130 via the above code, we create the\n",
        "    vocabulary and print its first 51 entries for illustration purposes.\n",
        "</div>"
      ],
      "metadata": {
        "id": "f87c2308-3363-4295-b3ad-36618216fd45"
      }
    },
    {
      "id": "9d7bf329-8f64-4aac-be16-446e1cb12dbe",
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "trusted": true,
        "id": "9d7bf329-8f64-4aac-be16-446e1cb12dbe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2e99fe4-2aeb-4f6e-a2b7-f34f59566afd",
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i>=300:\n",
        "        break"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e99fe4-2aeb-4f6e-a2b7-f34f59566afd",
        "outputId": "66f36228-5ad5-4a50-c334-7532e6fbd6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n",
            "('His', 51)\n",
            "('How', 52)\n",
            "('I', 53)\n",
            "('If', 54)\n",
            "('In', 55)\n",
            "('It', 56)\n",
            "('Jack', 57)\n",
            "('Jove', 58)\n",
            "('Just', 59)\n",
            "('Lord', 60)\n",
            "('Made', 61)\n",
            "('Miss', 62)\n",
            "('Money', 63)\n",
            "('Monte', 64)\n",
            "('Moon-dancers', 65)\n",
            "('Mr', 66)\n",
            "('Mrs', 67)\n",
            "('My', 68)\n",
            "('Never', 69)\n",
            "('No', 70)\n",
            "('Now', 71)\n",
            "('Nutley', 72)\n",
            "('Of', 73)\n",
            "('Oh', 74)\n",
            "('On', 75)\n",
            "('Once', 76)\n",
            "('Only', 77)\n",
            "('Or', 78)\n",
            "('Perhaps', 79)\n",
            "('Poor', 80)\n",
            "('Professional', 81)\n",
            "('Renaissance', 82)\n",
            "('Rickham', 83)\n",
            "('Riviera', 84)\n",
            "('Rome', 85)\n",
            "('Russian', 86)\n",
            "('Sevres', 87)\n",
            "('She', 88)\n",
            "('Stroud', 89)\n",
            "('Strouds', 90)\n",
            "('Suddenly', 91)\n",
            "('That', 92)\n",
            "('The', 93)\n",
            "('Then', 94)\n",
            "('There', 95)\n",
            "('They', 96)\n",
            "('This', 97)\n",
            "('Those', 98)\n",
            "('Though', 99)\n",
            "('Thwing', 100)\n",
            "('Thwings', 101)\n",
            "('To', 102)\n",
            "('Usually', 103)\n",
            "('Venetian', 104)\n",
            "('Victor', 105)\n",
            "('Was', 106)\n",
            "('We', 107)\n",
            "('Well', 108)\n",
            "('What', 109)\n",
            "('When', 110)\n",
            "('Why', 111)\n",
            "('Yes', 112)\n",
            "('You', 113)\n",
            "('_', 114)\n",
            "('a', 115)\n",
            "('abdication', 116)\n",
            "('able', 117)\n",
            "('about', 118)\n",
            "('above', 119)\n",
            "('abruptly', 120)\n",
            "('absolute', 121)\n",
            "('absorbed', 122)\n",
            "('absurdity', 123)\n",
            "('academic', 124)\n",
            "('accuse', 125)\n",
            "('accustomed', 126)\n",
            "('across', 127)\n",
            "('activity', 128)\n",
            "('add', 129)\n",
            "('added', 130)\n",
            "('admirers', 131)\n",
            "('adopted', 132)\n",
            "('adulation', 133)\n",
            "('advance', 134)\n",
            "('aesthetic', 135)\n",
            "('affect', 136)\n",
            "('afraid', 137)\n",
            "('after', 138)\n",
            "('afterward', 139)\n",
            "('again', 140)\n",
            "('ago', 141)\n",
            "('ah', 142)\n",
            "('air', 143)\n",
            "('alive', 144)\n",
            "('all', 145)\n",
            "('almost', 146)\n",
            "('alone', 147)\n",
            "('along', 148)\n",
            "('always', 149)\n",
            "('am', 150)\n",
            "('amazement', 151)\n",
            "('amid', 152)\n",
            "('among', 153)\n",
            "('amplest', 154)\n",
            "('amusing', 155)\n",
            "('an', 156)\n",
            "('and', 157)\n",
            "('another', 158)\n",
            "('answer', 159)\n",
            "('answered', 160)\n",
            "('any', 161)\n",
            "('anything', 162)\n",
            "('anywhere', 163)\n",
            "('apparent', 164)\n",
            "('apparently', 165)\n",
            "('appearance', 166)\n",
            "('appeared', 167)\n",
            "('appointed', 168)\n",
            "('are', 169)\n",
            "('arm', 170)\n",
            "('arm-chair', 171)\n",
            "('arm-chairs', 172)\n",
            "('arms', 173)\n",
            "('art', 174)\n",
            "('articles', 175)\n",
            "('artist', 176)\n",
            "('as', 177)\n",
            "('aside', 178)\n",
            "('asked', 179)\n",
            "('at', 180)\n",
            "('atmosphere', 181)\n",
            "('atom', 182)\n",
            "('attack', 183)\n",
            "('attention', 184)\n",
            "('attitude', 185)\n",
            "('audacities', 186)\n",
            "('away', 187)\n",
            "('awful', 188)\n",
            "('axioms', 189)\n",
            "('azaleas', 190)\n",
            "('back', 191)\n",
            "('background', 192)\n",
            "('balance', 193)\n",
            "('balancing', 194)\n",
            "('balustraded', 195)\n",
            "('basking', 196)\n",
            "('bath-rooms', 197)\n",
            "('be', 198)\n",
            "('beaming', 199)\n",
            "('bean-stalk', 200)\n",
            "('bear', 201)\n",
            "('beard', 202)\n",
            "('beauty', 203)\n",
            "('became', 204)\n",
            "('because', 205)\n",
            "('becoming', 206)\n",
            "('bed', 207)\n",
            "('been', 208)\n",
            "('before', 209)\n",
            "('began', 210)\n",
            "('begun', 211)\n",
            "('behind', 212)\n",
            "('being', 213)\n",
            "('believed', 214)\n",
            "('beneath', 215)\n",
            "('bespoke', 216)\n",
            "('better', 217)\n",
            "('between', 218)\n",
            "('big', 219)\n",
            "('bits', 220)\n",
            "('bitterness', 221)\n",
            "('blocked', 222)\n",
            "('born', 223)\n",
            "('borne', 224)\n",
            "('boudoir', 225)\n",
            "('bravura', 226)\n",
            "('break', 227)\n",
            "('breaking', 228)\n",
            "('breathing', 229)\n",
            "('bric-a-brac', 230)\n",
            "('briefly', 231)\n",
            "('brings', 232)\n",
            "('bronzes', 233)\n",
            "('brought', 234)\n",
            "('brown', 235)\n",
            "('brush', 236)\n",
            "('bull', 237)\n",
            "('business', 238)\n",
            "('but', 239)\n",
            "('buying', 240)\n",
            "('by', 241)\n",
            "('called', 242)\n",
            "('came', 243)\n",
            "('can', 244)\n",
            "('canvas', 245)\n",
            "('canvases', 246)\n",
            "('cards', 247)\n",
            "('care', 248)\n",
            "('career', 249)\n",
            "('caught', 250)\n",
            "('central', 251)\n",
            "('chair', 252)\n",
            "('chap', 253)\n",
            "('characteristic', 254)\n",
            "('charming', 255)\n",
            "('cheap', 256)\n",
            "('check', 257)\n",
            "('cheeks', 258)\n",
            "('chest', 259)\n",
            "('chimney-piece', 260)\n",
            "('chucked', 261)\n",
            "('cigar', 262)\n",
            "('cigarette', 263)\n",
            "('cigars', 264)\n",
            "('circulation', 265)\n",
            "('circumstance', 266)\n",
            "('circus-clown', 267)\n",
            "('claimed', 268)\n",
            "('clasping', 269)\n",
            "('clear', 270)\n",
            "('cleverer', 271)\n",
            "('close', 272)\n",
            "('clue', 273)\n",
            "('coat', 274)\n",
            "('collapsed', 275)\n",
            "('colour', 276)\n",
            "('come', 277)\n",
            "('comfortable', 278)\n",
            "('coming', 279)\n",
            "('companion', 280)\n",
            "('compared', 281)\n",
            "('complex', 282)\n",
            "('confident', 283)\n",
            "('congesting', 284)\n",
            "('conjugal', 285)\n",
            "('constraint', 286)\n",
            "('consummate', 287)\n",
            "('contended', 288)\n",
            "('continued', 289)\n",
            "('corner', 290)\n",
            "('corrected', 291)\n",
            "('could', 292)\n",
            "('couldn', 293)\n",
            "('count', 294)\n",
            "('countenance', 295)\n",
            "('couple', 296)\n",
            "('course', 297)\n",
            "('covered', 298)\n",
            "('craft', 299)\n",
            "('cried', 300)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "77f4ddba-48da-411c-8ebf-9d074b598002",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff9c4; padding: 10px; border-left: 5px solid #fbc02d;\">\n",
        "    <b>Note:</b> As we can see, based on the output above, the dictionary contains individual tokens\n",
        "    associated with unique integer labels.\n",
        "</div>"
      ],
      "metadata": {
        "id": "77f4ddba-48da-411c-8ebf-9d074b598002"
      }
    },
    {
      "id": "57496f2f-51f6-4285-ac40-1b615069e043",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#d0f0c0; padding: 10px; border-left: 5px solid #388e3c;\">\n",
        "    <b>Note:</b> Later in this book, when we want to convert the outputs of an LLM from numbers back into\n",
        "    text, we also need a way to turn token IDs into text. <br><br>\n",
        "    For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
        "</div>"
      ],
      "metadata": {
        "id": "57496f2f-51f6-4285-ac40-1b615069e043"
      }
    },
    {
      "id": "e389335d-f299-4479-b2b9-836a46a2da9e",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e3f2fd; padding: 10px; border-left: 5px solid #1e88e5;\">\n",
        "    <b>Note:</b> Let's implement a complete tokenizer class in Python. <br><br>\n",
        "    The class will have an <code>encode</code> method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. <br><br>\n",
        "    In addition, we implement a <code>decode</code> method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.\n",
        "</div>"
      ],
      "metadata": {
        "id": "e389335d-f299-4479-b2b9-836a46a2da9e"
      }
    },
    {
      "id": "1b9ae355-2a72-4299-9771-cc6a4a34d05e",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #8e24aa;\">\n",
        "    <b>Steps:</b><br>\n",
        "    Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods<br>\n",
        "    Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens<br>\n",
        "    Step 3: Process input text into token IDs<br>\n",
        "    Step 4: Convert token IDs back into text<br>\n",
        "    Step 5: Replace spaces before the specified punctuation\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "1b9ae355-2a72-4299-9771-cc6a4a34d05e"
      }
    },
    {
      "id": "56a3817d-18d2-498a-a25e-7c6fa62d33e3",
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'(\\.\\.\\.|--|[.,;:?!\"()\\']|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        #Replace spaces vefore the specified punctuaions\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "trusted": true,
        "id": "56a3817d-18d2-498a-a25e-7c6fa62d33e3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3d45d6f0-d29d-409c-881b-6ac4cf31abf0",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff3e0; padding: 10px; border-left: 5px solid #fb8c00;\">\n",
        "    Let's instantiate a new tokenizer object from the <code>SimpleTokenizerV1</code> class and tokenize a\n",
        "    passage from Edith Wharton's short story to try it out in practice:\n",
        "</div>"
      ],
      "metadata": {
        "id": "3d45d6f0-d29d-409c-881b-6ac4cf31abf0"
      }
    },
    {
      "id": "6c17438a-74f8-4387-ac08-033c928911e5",
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c17438a-74f8-4387-ac08-033c928911e5",
        "outputId": "1b9c7886-6a96-4480-ef8b-f63bad83aee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "21d2ad40-2cb8-4bb4-8c1c-bac97522b452",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e8f5e9; padding: 10px;\n",
        "border-left: 5px solid #43a047;\">\n",
        "    <b>Note:</b> The code above prints the following token IDs:<br>\n",
        "    Next, let's see if we can turn these token IDs back into text using the <code>decode</code> method.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "21d2ad40-2cb8-4bb4-8c1c-bac97522b452"
      }
    },
    {
      "id": "9771cacb-6ac6-415e-afab-20fa304606e3",
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9771cacb-6ac6-415e-afab-20fa304606e3",
        "outputId": "e1e21e07-12bc-4c7c-baa0-69c481144edc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "execution_count": null
    },
    {
      "id": "c87d56f1-a927-4f9f-9587-037bec7b2020",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff9c4; padding: 10px; border-left: 5px solid #fbc02d;\">\n",
        "    <b>Note:</b> Based on the output above, we can see that the <code>decode</code> method successfully converted the\n",
        "    token IDs back into the original text.\n",
        "</div>"
      ],
      "metadata": {
        "id": "c87d56f1-a927-4f9f-9587-037bec7b2020"
      }
    },
    {
      "id": "5c5dc0d0-31d7-4a10-b728-0bafa6609134",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e3f2fd; padding: 10px; border-left: 5px solid #2196f3;\">\n",
        "    <b>Note:</b> So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing\n",
        "    text based on a snippet from the training set. <br><br>\n",
        "    Let's now apply it to a new text sample that is not contained in the training set:\n",
        "</div>"
      ],
      "metadata": {
        "id": "5c5dc0d0-31d7-4a10-b728-0bafa6609134"
      }
    },
    {
      "id": "b906b02d-9f57-4aaa-885a-eb4126ef9222",
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "b906b02d-9f57-4aaa-885a-eb4126ef9222",
        "outputId": "c241c51d-daa6-41c8-b69f-82b6e3d60769"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1763555282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3743522749.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(\\.\\.\\.|--|[.,;:?!\"()\\']|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3743522749.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(\\.\\.\\.|--|[.,;:?!\"()\\']|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "3655337b-22d3-468b-b8da-0008322f2070",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #9c27b0;\">\n",
        "    <b>Note:</b> The problem is that the word \"Hello\" was not used in the <i>The Verdict</i> short story. <br><br>\n",
        "    Hence, it is not contained in the vocabulary. <br><br>\n",
        "    This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs.\n",
        "</div>"
      ],
      "metadata": {
        "id": "3655337b-22d3-468b-b8da-0008322f2070"
      }
    },
    {
      "id": "4c4c3e80-08e5-4a67-9e73-36151d019186",
      "cell_type": "markdown",
      "source": [
        "# ADDING SPECIAL CONTEXT TOKENS\n",
        "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
        "from the training set.\n",
        "\n",
        "In this section, we will modify this tokenizer to handle unknown\n",
        "words.\n",
        "\n",
        "\n",
        "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
        "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
        "<|endoftext|>"
      ],
      "metadata": {
        "id": "4c4c3e80-08e5-4a67-9e73-36151d019186"
      }
    },
    {
      "id": "47f975f2-796e-4d6a-b105-ec5c4d2b6d49",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff3e0; padding: 10px; border-left: 5px solid #fb8c00;\">\n",
        "    <b>Note:</b> We can modify the tokenizer to use an <code>&lt;|unk|&gt;</code> token if it encounters a word that is not part of the vocabulary. <br><br>\n",
        "    Furthermore, we add a token between unrelated texts. <br><br>\n",
        "    For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source.\n",
        "</div>"
      ],
      "metadata": {
        "id": "47f975f2-796e-4d6a-b105-ec5c4d2b6d49"
      }
    },
    {
      "id": "237e176f-807a-4aad-8e14-1277f95dfc1e",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fce4ec; padding: 10px; border-left: 5px solid #f06292;\">\n",
        "    <b>Note:</b> Let's now modify the vocabulary to include these two special tokens, <code>&lt;unk&gt;</code> and <code>&lt;|endoftext|&gt;</code>, by adding these to the list of all unique words that we created in the previous section.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "237e176f-807a-4aad-8e14-1277f95dfc1e"
      }
    },
    {
      "id": "a6cc522d-a0fd-44d8-ad64-0108fa7db4f8",
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "trusted": true,
        "id": "a6cc522d-a0fd-44d8-ad64-0108fa7db4f8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5edd3242-81d1-420f-916f-e2c6f4bba02e",
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5edd3242-81d1-420f-916f-e2c6f4bba02e",
        "outputId": "c3b3a47d-aa04-4b44-b56c-a599c0d9e193"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "execution_count": null
    },
    {
      "id": "67db441b-607d-4e3f-9817-35f14d464546",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff9c4; padding: 10px; border-left: 5px solid #fbc02d;\">\n",
        "    <b>Note:</b> Based on the output of the print statement above, the new vocabulary size is 1132 (the vocabulary size in the previous section was 1130).\n",
        "</div>"
      ],
      "metadata": {
        "id": "67db441b-607d-4e3f-9817-35f14d464546"
      }
    },
    {
      "id": "87c6cc36-9e20-4dc2-9c64-f148c780946f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eeeeee; padding: 10px; border-left: 5px solid #9e9e9e;\">\n",
        "    <b>Note:</b> As an additional quick check, let's print the last 5 entries of the updated vocabulary.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "87c6cc36-9e20-4dc2-9c64-f148c780946f"
      }
    },
    {
      "id": "91c14e87-f588-4f43-af85-92126506ca7f",
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c14e87-f588-4f43-af85-92126506ca7f",
        "outputId": "8b78291d-5de4-4e29-da48-8b1247001e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "f829fe62-7a73-4287-8a2a-94f9bab97de0",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e8f5e9; padding: 10px; border-left: 5px solid #43a047;\">\n",
        "    <b>Note:</b> A simple text tokenizer that handles unknown words.\n",
        "</div>"
      ],
      "metadata": {
        "id": "f829fe62-7a73-4287-8a2a-94f9bab97de0"
      }
    },
    {
      "id": "b7eeec1f-e493-4742-9c3b-e2c0a2bca947",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e3f2fd; padding: 10px; border-left: 5px solid #1e88e5;\">\n",
        "    <b>Info:</b>\n",
        "    <ul style=\"margin: 0; padding-left: 20px;\">\n",
        "        <li>Step 1: Replace unknown words by <code>&lt;|unk|&gt;</code> tokens</li>\n",
        "        <li>Step 2: Replace spaces before the specified punctuations</li>\n",
        "    </ul>\n",
        "</div>"
      ],
      "metadata": {
        "id": "b7eeec1f-e493-4742-9c3b-e2c0a2bca947"
      }
    },
    {
      "id": "5fa74605-b59e-47d1-8e21-6a2509fbb805",
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'(\\.\\.\\.|--|[.,;:?!\"()\\']|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        #Replace spaces vefore the specified punctuaions\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "trusted": true,
        "id": "5fa74605-b59e-47d1-8e21-6a2509fbb805"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "98405bba-834e-4b11-bd1d-7a5477898a6c",
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98405bba-834e-4b11-bd1d-7a5477898a6c",
        "outputId": "0e2e6583-88c5-4de7-9c3d-d1c114ddd465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "9783f8ce-518a-4831-89fa-b33d9e00030a",
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9783f8ce-518a-4831-89fa-b33d9e00030a",
        "outputId": "4bafe741-40e5-4178-cd64-47f4add1d0a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "execution_count": null
    },
    {
      "id": "18e6fedc-408b-4260-993f-79498acea426",
      "cell_type": "code",
      "source": [
        "tokenizer.decode (tokenizer.encode(text))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "18e6fedc-408b-4260-993f-79498acea426",
        "outputId": "490e3836-af18-45ec-e824-80a557a277ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "execution_count": null
    },
    {
      "id": "95299169-f284-417f-ae6d-df22f078cfd4",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#d0f0c0; padding: 10px; border-left: 5px solid #4caf50;\">\n",
        "    <b>Info:</b> Based on comparing the de-tokenized text above with the original input text, we know that\n",
        "    the training dataset, Edith Wharton's short story <i>The Verdict</i>, did not contain the words\n",
        "    <code>\"Hello\"</code> and <code>\"palace\"</code>.\n",
        "</div>"
      ],
      "metadata": {
        "id": "95299169-f284-417f-ae6d-df22f078cfd4"
      }
    },
    {
      "id": "8c7a47ef-3ae6-42be-93f2-a123fcdd539f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#e0f7fa; padding: 10px; border-left: 5px solid #0097a7;\">\n",
        "  <b>Info:</b> So far, we have discussed tokenization as an essential step in processing text as input to\n",
        "  LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
        "  as the following:\n",
        "  <ul>\n",
        "    <li><b>[BOS]</b> (beginning of sequence): This token marks the start of a text. It\n",
        "    signifies to the LLM where a piece of content begins.</li>\n",
        "    <li><b>[EOS]</b> (end of sequence): This token is positioned at the end of a text,\n",
        "    and is especially useful when concatenating multiple unrelated texts,\n",
        "    similar to <code>&lt;|endoftext|&gt;</code>. For instance, when combining two different\n",
        "    Wikipedia articles or books, the [EOS] token indicates where one article\n",
        "    ends and the next one begins.</li>\n",
        "    <li><b>[PAD]</b> (padding): When training LLMs with batch sizes larger than one,\n",
        "    the batch might contain texts of varying lengths. To ensure all texts have\n",
        "    the same length, the shorter texts are extended or \"padded\" using the\n",
        "    [PAD] token, up to the length of the longest text in the batch.</li>\n",
        "  </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "8c7a47ef-3ae6-42be-93f2-a123fcdd539f"
      }
    },
    {
      "id": "829b9e91-f04a-4840-bf25-e7d780625f4f",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#f3e5f5; padding: 10px; border-left: 5px solid #9c27b0;\">\n",
        "  <b>Note:</b> Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
        "  above but only uses an <code>&lt;|endoftext|&gt;</code> token for simplicity.\n",
        "</div>"
      ],
      "metadata": {
        "id": "829b9e91-f04a-4840-bf25-e7d780625f4f"
      }
    },
    {
      "id": "88c9df3b-b7c3-48da-9dc2-02f9a059c4ec",
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#fff3e0; padding: 10px; border-left: 5px solid #fb8c00;\">\n",
        "  <b>Info:</b> The tokenizer used for GPT models also doesn't use an <code>&lt;|unk|&gt;</code> token for out-of-vocabulary words.\n",
        "  Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "88c9df3b-b7c3-48da-9dc2-02f9a059c4ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KBldn7N5eIr",
        "outputId": "bd3ac172-7658-41e3-909f-78f454844948"
      },
      "id": "4KBldn7N5eIr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "B6cIzeGq55F1"
      },
      "id": "B6cIzeGq55F1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe0b2; padding: 10px; border-left: 5px solid #fb8c00;\">\n",
        "    <b>Note:</b> The usage of this tokenizer is similar to <code>SimpleTokenizerV2</code> we implemented previously via an <code>encode</code> method.\n",
        "</div>"
      ],
      "metadata": {
        "id": "ITpD8QEN-W3U"
      },
      "id": "ITpD8QEN-W3U"
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit of terraces\"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JqSjIwC5_3a",
        "outputId": "b6feafd2-a20c-427e-ad0f-8b65282865de"
      },
      "id": "4JqSjIwC5_3a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 286, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above prints the following token IDs:"
      ],
      "metadata": {
        "id": "4_X-f7_g_DeU"
      },
      "id": "4_X-f7_g_DeU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then convert the token IDs back into text using the decode medthod, similar to our SimpleTokenizerV2 earlier."
      ],
      "metadata": {
        "id": "JGdRRLVB_m0K"
      },
      "id": "JGdRRLVB_m0K"
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTACmAvI_xBy",
        "outputId": "fe8f2a7a-dfb7-489d-edb9-683f47752a4b"
      },
      "id": "XTACmAvI_xBy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit of terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** We can make two noteworthy ovbservations based on the token IDs and decoded text above.\n",
        "\n",
        "First, the <|endoftext|> token is assigned a relativelt large token IDs, namely, 50256.\n",
        "\n",
        "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary soze of 50,257\n",
        "with <|endoftext|> being assigned the largest token ID.\n"
      ],
      "metadata": {
        "id": "Mj7b9LqPCQAR"
      },
      "id": "Mj7b9LqPCQAR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Second, the BPE tokenizer above encodes and decodes unknown words, such as\n",
        "\"someunknownPlace\" correctly.\n",
        "\n",
        "The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens?"
      ],
      "metadata": {
        "id": "fRWiwp9cDNCy"
      },
      "id": "fRWiwp9cDNCy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary\n",
        "into smaller subword units or even individual characters.\n",
        "\n",
        "The enables it to handle out-ofvocabulary words.\n",
        "\n",
        "So, thanks to the BPE algorithm, if the tokenizer encounters an\n",
        "unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or\n",
        "characters"
      ],
      "metadata": {
        "id": "PmdXkXyRDhbl"
      },
      "id": "PmdXkXyRDhbl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let us take another simple example to illustrate how the BPE tokenizer deals with unknown tokens**"
      ],
      "metadata": {
        "id": "LptimRAxDl1F"
      },
      "id": "LptimRAxDl1F"
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"Ayush Morya\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnN0KjxLDgzS",
        "outputId": "9a9c7548-a05b-40d5-9651-09c14c6870fb"
      },
      "id": "FnN0KjxLDgzS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42012, 1530, 337, 652, 64]\n",
            "Ayush Morya\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1P3U2nLDqsy",
        "outputId": "77860f70-d28f-4d4b-f824-3c0715d6eabb"
      },
      "id": "W1P3U2nLDqsy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CREATING INPUT-TARGET PAIRS**"
      ],
      "metadata": {
        "id": "Jx89p8RozNQW"
      },
      "id": "Jx89p8RozNQW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach."
      ],
      "metadata": {
        "id": "HJwmJgDkzUjW"
      },
      "id": "HJwmJgDkzUjW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get started, we will first tokenize the whole The Verdict short story we worked with earlier using the BPE tokenizer introduced in the previous sections."
      ],
      "metadata": {
        "id": "Z3-WDoGgziS1"
      },
      "id": "Z3-WDoGgziS1"
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE1B1J0pywd6",
        "outputId": "7b380183-2715-4ccc-c43e-454a352e2915"
      },
      "id": "RE1B1J0pywd6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the code above will return 5145, the total number of tokens in the training set, after applying the BPE tokenizer."
      ],
      "metadata": {
        "id": "2BiW68Eu0MQ3"
      },
      "id": "2BiW68Eu0MQ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it results in a slightly more intersting text passage in the next steps."
      ],
      "metadata": {
        "id": "h_3kuFim05CI"
      },
      "id": "h_3kuFim05CI"
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]\n",
        "print (enc_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs_-zf650AhG",
        "outputId": "40a10120-032a-417c-abe0-47abc21f5d88"
      },
      "id": "Vs_-zf650AhG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469, 438, 14363, 938, 4842, 1650, 353, 438, 2934, 489, 3255, 465, 48422, 540, 450, 67, 3299, 13, 366, 5189, 1781, 340, 338, 1016, 284, 3758, 262, 1988, 286, 616, 4286, 705, 1014, 510, 26, 475, 314, 836, 470, 892, 286, 326, 11, 1770, 13, 8759, 2763, 438, 1169, 2994, 284, 943, 17034, 318, 477, 314, 892, 286, 526, 383, 1573, 11, 319, 9074, 13, 536, 5469, 338, 11914, 11, 33096, 663, 4808, 3808, 62, 355, 996, 484, 547, 12548, 287, 281, 13079, 410, 12523, 286, 22353, 13, 843, 340, 373, 407, 691, 262, 9074, 13, 536, 48819, 508, 25722, 276, 13, 11161, 407, 262, 40123, 18113, 544, 9325, 701, 11, 379, 262, 938, 402, 1617, 261, 12917, 905, 11, 5025, 502, 878, 402, 271, 10899, 338, 366, 31640, 12, 67, 20811, 1, 284, 910, 11, 351, 10953, 287, 607, 2951, 25, 366, 1135, 2236, 407, 804, 2402, 663, 588, 757, 13984, 198, 198, 5779, 28112, 10197, 832, 262, 46475, 286, 18113, 544, 338, 10953, 314, 2936, 1498, 284, 1986, 262, 1109, 351, 1602, 11227, 414, 13, 23676, 3619, 402, 271, 10899, 0, 383, 1466, 550, 925, 683, 438, 270, 373, 15830, 326, 484, 815, 25722, 683, 13, 9754, 465, 898, 1714, 7380, 30090, 547, 2982, 11, 290, 287, 465, 898, 3292, 8941, 257, 4636, 28582, 13, 18612, 35394, 30, 8673, 13, 1002, 340, 547, 11, 262, 15393, 286, 262, 5977, 373, 29178, 3474, 416, 1310, 40559, 11959, 1636, 11, 508, 11, 287, 477, 922, 4562, 11, 3181, 503, 287, 262, 37090, 257, 845, 22665, 366, 672, 270, 2838, 1, 319, 3619, 438, 505, 286, 883, 905, 88, 6685, 42070, 351, 4738, 6276, 871, 326, 314, 423, 2982, 357, 40, 1839, 470, 910, 416, 4150, 8, 3688, 284, 402, 271, 10899, 338, 12036, 13, 843, 523, 438, 14363, 10568, 852, 5729, 11331, 18893, 540, 438, 1169, 5114, 11835, 3724, 503, 11, 290, 11, 355, 9074, 13, 536, 5469, 550, 11001, 11, 262, 2756, 286, 366, 38, 271, 10899, 82, 1, 1816, 510, 13, 198, 198, 1026, 373, 407, 10597, 1115, 812, 1568, 326, 11, 287, 262, 1781, 286, 257, 1178, 2745, 6, 4686, 1359, 319, 262, 34686, 41976, 11, 340, 6451, 5091, 284, 502, 284, 4240, 1521, 402, 271, 10899, 550, 1813, 510, 465, 12036, 13, 1550, 14580, 11, 340, 1107, 373, 257, 29850, 1917, 13, 1675, 24456, 465, 3656, 561, 423, 587, 1165, 2562, 438, 14363, 3148, 1650, 1010, 550, 587, 6699, 262, 1540, 558, 286, 2282, 326, 9074, 13, 402, 271, 10899, 550, 366, 7109, 14655, 683, 866, 526, 1114, 9074, 13, 402, 271, 10899, 438, 292, 884, 438, 18108, 407, 11196, 10597, 3016, 257, 614, 706, 3619, 338, 10568, 550, 587, 2077, 13, 632, 1244, 307, 326, 339, 550, 6405, 607, 438, 20777, 339, 8288, 465, 10152, 438, 13893, 339, 1422, 470, 765, 284, 467, 319, 12036, 26, 475, 340, 561, 423, 587, 1327, 284, 5879, 326, 339, 550, 1813, 510, 465, 12036, 780, 339, 550, 6405, 607, 13, 198, 198, 5189, 1781, 11, 611, 673, 550, 407, 17901, 683, 866, 11, 673, 550, 8603, 11, 355, 4544, 9325, 701, 42397, 11, 4054, 284, 366, 26282, 683, 510, 1, 438, 7091, 550, 407, 2957, 683, 736, 284, 262, 1396, 417, 13, 1675, 1234, 262, 14093, 656, 465, 1021, 757, 438, 10919, 257, 410, 5040, 329, 257, 3656, 0, 887, 9074, 13, 402, 271, 10899, 4120, 284, 423, 595, 67, 1328, 340, 438, 392, 314, 2936, 340, 1244, 307, 3499, 284, 1064, 503, 1521, 13, 198, 198, 464, 748, 586, 652, 1204, 286, 262, 34686, 41976, 37733, 2346, 284, 884, 14177, 8233, 1020, 5768, 26, 290, 1719, 11, 319, 616, 835, 284, 22489, 40089, 11, 4978, 257, 19350, 286, 3619, 338, 3652, 436, 81, 5286, 8812, 2114, 1022, 262, 279, 1127, 11, 314, 550, 3589, 28068, 294, 1555, 262, 1306, 1110, 13, 198, 198, 40, 1043, 262, 3155, 379, 8887, 11061, 511, 18057, 12, 83, 6037, 26, 290, 9074, 13, 402, 271, 10899, 338, 7062, 373, 523, 2429, 498, 326, 11, 287, 262, 29543, 2745, 11, 314, 4752, 340, 6777, 13, 632, 373, 407, 326, 616, 2583, 408, 373, 366, 47914, 1298, 319, 326, 966, 314, 714, 423, 1813, 4544, 9325, 701, 262, 40830, 12719, 3874, 13, 632, 373, 655, 780, 673, 373, 4808, 1662, 62, 3499, 438, 361, 314, 743, 307, 41746, 12004, 262, 6473, 438, 5562, 314, 1043, 607, 523, 13, 1114, 3619, 11, 477, 465, 1204, 11, 550, 587, 11191, 416, 3499, 1466, 25, 484, 550, 26546, 1068, 465, 1242, 11, 340, 550, 587, 302, 1144, 287, 262, 3024, 12, 4803, 286, 511, 512, 1741, 13, 843, 340, 373, 4361, 5048, 425, 284, 3465, 644, 1245, 262, 366, 25124, 3101, 8137, 286, 16957, 1696, 414, 1, 357, 40, 9577, 4544, 9325, 701, 8, 373, 1719, 319, 683, 13, 198, 198, 40, 423, 4750, 326, 9074, 13, 402, 271, 10899, 373, 5527, 26, 290, 340, 373, 3393, 34953, 856, 326, 607, 5229, 373, 37895, 422, 428, 25179, 257, 19217, 475, 8904, 14676, 13, 632, 318, 11, 355, 257, 3896, 11, 262, 661, 508, 40987, 1637, 508, 651, 749, 503, 286, 340, 26, 290, 3619, 338, 19992, 31564, 286, 465, 3656, 338, 1263, 5236, 9343, 683, 11, 351, 281, 5585, 286, 2818, 922, 12, 49705, 11, 284, 21595, 1133, 340, 656, 5563, 286, 1242, 290, 13064, 13, 1675, 262, 6846, 11, 314, 1276, 751, 11, 339, 6150, 5365, 31655, 26, 475, 339, 373, 7067, 29396, 18443, 12271, 290, 45592, 12, 14792, 5986, 351, 257, 8839, 326, 7284, 35924, 262, 12306, 395, 4133, 13, 198, 198, 1, 26788, 338, 691, 12226, 318, 284, 1234, 8737, 656, 19133, 553, 373, 530, 286, 262, 7877, 72, 3150, 339, 8104, 866, 1973, 262, 37918, 411, 290, 8465, 286, 281, 33954, 271, 3973, 9899, 14678, 40556, 12, 11487, 11, 618, 11, 319, 257, 1568, 1110, 11, 314, 550, 757, 1057, 625, 422, 22489, 40089, 26, 290, 9074, 13, 402, 271, 10899, 11, 307, 3723, 319, 683, 11, 2087, 329, 616, 35957, 25, 366, 14295, 318, 523, 34813, 306, 8564, 284, 790, 1296, 286, 8737, 526, 198, 198, 43920, 3619, 0, 632, 550, 1464, 587, 465, 10030, 284, 423, 1466, 910, 884, 1243, 286, 683, 25, 262, 1109, 815, 307, 900, 866, 287, 1070, 268, 2288, 13, 1867, 7425, 502, 783, 373, 326, 11, 329, 262, 717, 640, 11, 339, 581, 4714, 262, 8216, 13, 314, 550, 1775, 683, 11, 523, 1690, 11, 1615, 3364, 739, 2092, 256, 7657, 438, 9776, 340, 262, 11644, 43778, 3465, 326, 26773, 606, 286, 511, 6799, 454, 30, 1400, 438, 1640, 11, 31414, 1576, 11, 340, 2627, 4156, 326, 339, 373, 16245, 286, 9074, 13, 402, 271, 10899, 438, 69, 623, 1576, 407, 284, 766, 607, 41793, 13, 632, 373, 465, 898, 41793, 339, 3947, 284, 307, 1592, 2259, 739, 438, 14363, 898, 9408, 355, 281, 2134, 329, 5482, 4447, 290, 753, 1072, 13, 198, 198, 1, 3666, 13674, 11, 1201, 314, 1053, 442, 17758, 12036, 661, 836, 470, 910, 326, 3404, 546, 502, 438, 9930, 910, 340, 546, 12622, 41379, 293, 553, 373, 465, 691, 5402, 11, 355, 339, 8278, 422, 262, 3084, 290, 336, 8375, 503, 4291, 262, 4252, 18250, 8812, 558, 13, 198, 198, 40, 27846, 706, 683, 11, 7425, 416, 465, 938, 1573, 13, 12622, 41379, 293, 373, 11, 287, 1109, 11, 5033, 262, 582, 286, 262, 2589, 438, 292, 3619, 2241, 11, 530, 1244, 1234, 340, 11, 550, 587, 262, 582, 286, 262, 1711, 13, 383, 7099, 6802, 373, 531, 284, 423, 7042, 2241, 379, 616, 1545, 338, 3625, 11, 290, 314, 14028, 611, 257, 256, 11912, 286, 35394, 739, 10724, 262, 6846, 338, 11428, 450, 67, 3299, 13, 887, 645, 438, 1640, 340, 373, 407, 10597, 706, 326, 1785, 326, 262, 4808, 13698, 10322, 6532, 62, 8263, 12, 9649, 550, 9258, 284, 3359, 511, 366, 8642, 521, 829, 526, 198, 198, 40, 2900, 284, 9074, 13, 402, 271, 10899, 11, 508, 550, 18459, 1068, 284, 1577, 257, 23844, 286, 7543, 284, 607, 599, 6321, 287, 262, 17423, 12, 3823, 13, 198, 198, 1, 5195, 4808, 10134, 62, 339, 442, 17758, 12036, 1701, 314, 1965, 25891, 13, 198, 198, 3347, 4376, 607, 26928, 351, 257, 9254, 286, 922, 12, 17047, 8167, 5975, 13, 198, 198, 1, 5812, 11, 339, 1595, 470, 4808, 14150, 62, 284, 783, 11, 345, 760, 26, 290, 314, 765, 683, 284, 2883, 2241, 553, 673, 531, 2407, 2391, 13, 198, 198, 40, 3114, 546, 262, 40894, 2330, 12, 6839, 11978, 2119, 11, 351, 663, 4808, 44769, 8270, 12, 332, 660, 62, 410, 1386, 20394, 262, 23755, 286, 262, 14005, 1801, 2093, 41160, 11, 290, 663, 45592, 12, 14792, 1613, 1424, 287, 19217, 24887, 13431, 13, 198, 198, 1, 19242, 339, 442, 17758, 465, 5986, 1165, 30, 314, 4398, 470, 1775, 257, 2060, 530, 287, 262, 2156, 526, 198, 198, 32, 3731, 17979, 286, 32315, 12606, 9074, 13, 402, 271, 10899, 338, 1280, 954, 36368, 13, 366, 1026, 338, 465, 11441, 48740, 11, 345, 760, 13, 679, 1139, 484, 821, 407, 4197, 284, 423, 546, 26, 339, 338, 1908, 606, 477, 1497, 2845, 530, 438, 1820, 18560, 438, 392, 326, 314, 423, 284, 1394, 26148, 526, 198, 198, 6653, 11441, 48740, 438, 14295, 338, 48740, 546, 465, 5986, 30, 2011, 20136, 373, 3957, 588, 262, 26394, 12, 301, 971, 13, 314, 531, 10722, 292, 2280, 284, 616, 2583, 408, 25, 366, 40, 1276, 1107, 766, 534, 18560, 11, 345, 760, 526, 198, 198, 3347, 27846, 503, 2048, 4628, 24882, 379, 262, 8812, 558, 810, 607, 5229, 11, 21081, 782, 278, 287, 257, 14263, 276, 5118, 11, 550, 6578, 257, 24518, 290, 7428, 262, 3394, 20096, 39047, 338, 1182, 1022, 465, 14475, 13, 198, 198, 1, 5779, 11, 1282, 981, 339, 338, 407, 2045, 553, 673, 531, 11, 351, 257, 6487, 326, 3088, 284, 7808, 607, 10927, 1108, 26, 290, 314, 3940, 607, 1022, 262, 30623, 2295, 49406, 286, 262, 6899, 11, 290, 510, 262, 3094, 16046, 351, 1059, 430, 12, 66, 12375, 299, 20896, 82, 24357, 1871, 12734, 379, 1123, 9581, 13, 198, 198, 818, 262, 5391, 76, 395, 5228, 286, 607, 275, 2778, 10840, 11, 10371, 257, 1534, 4241, 286, 19217, 290, 18876, 5563, 11, 9174, 530, 286, 262, 5385, 41186, 39614, 1386, 11, 287, 262, 13203, 5482, 1044, 276, 5739, 13, 383, 5019, 19001, 286, 262, 5739, 1444, 510, 477, 402, 271, 10899, 338, 1613, 0, 198, 198, 27034, 13, 402, 271, 10899, 9859, 736, 262, 4324, 12, 66, 3325, 1299, 11, 3888, 7263, 257, 4808, 73, 446, 259, 13235, 62, 1336, 286, 11398, 35560, 1000, 292, 11, 7121, 281, 3211, 12, 16337, 1497, 11, 290, 531, 25, 366, 1532, 345, 1302, 994, 345, 460, 655, 6687, 284, 766, 340, 13, 314, 550, 340, 625, 262, 24818, 417, 12, 12239, 11, 475, 339, 3636, 470, 1309, 340, 2652, 526, 198, 198, 5297, 438, 40, 714, 655, 6687, 284, 766, 340, 438, 1169, 717, 18560, 286, 3619, 338, 314, 550, 1683, 550, 284, 14022, 616, 2951, 625, 0, 19672, 484, 550, 262, 1295, 286, 15393, 438, 16706, 262, 4318, 6103, 287, 257, 14005, 7872, 393, 4808, 13698, 10322, 6532, 62, 8263, 12, 3823, 11, 393, 257, 36364, 1396, 417, 4624, 523, 326, 340, 1718, 262, 1657, 832, 41160, 286, 1468, 9932, 316, 666, 966, 13, 383, 517, 12949, 1295, 2627, 262, 4286, 1365, 26, 1865, 11, 355, 616, 2951, 6348, 23840, 284, 262, 2063, 12, 2971, 11, 477, 262, 16704, 14482, 1625, 503, 438, 439, 262, 10818, 20597, 32192, 355, 2709, 330, 871, 11, 262, 15910, 286, 16153, 312, 328, 3780, 416, 543, 11, 351, 884, 2784, 9830, 5032, 11, 339, 5257, 284, 36583, 3241, 422, 262, 1103, 1597, 286, 262, 4286, 284, 617, 2495, 11331, 2768, 590, 286, 3703, 13, 9074, 13, 402, 271, 10899, 11, 17728, 257, 8500, 4417, 284, 670, 319, 438, 15464, 11, 355, 340, 547, 11, 523, 16857, 262, 4469, 286, 607, 898, 4286, 438, 18108, 26269, 5223, 287, 281, 8468, 4922, 284, 262, 3359, 286, 428, 3991, 4118, 84, 16579, 13, 383, 4286, 373, 530, 286, 3619, 338, 366, 11576, 395, 553, 355, 465, 21099, 3808, 561, 423, 1234, 340, 438, 270, 7997, 11, 319, 465, 636, 11, 257, 29844, 286, 12749, 11, 257, 22791, 278, 286, 32375, 11, 257, 22486, 11, 965, 2860, 1359, 290, 965, 1397, 11, 326, 14516, 530, 286, 262, 33125, 12, 565, 593, 338, 25304, 4040, 284, 10303, 257, 17972, 13, 632, 1138, 11, 287, 1790, 11, 379, 790, 966, 262, 3512, 286, 14081, 2415, 284, 307, 13055, 366, 11576, 306, 1, 780, 673, 373, 10032, 286, 852, 13055, 366, 34751, 306, 1, 438, 392, 1865, 407, 284, 4425, 281, 22037, 286, 262, 32073, 13, 198, 198, 1, 1026, 338, 262, 938, 339, 13055, 11, 345, 760, 553, 9074, 13, 402, 271, 10899, 531, 351, 27322, 540, 11293, 13, 366, 464, 938, 475, 530, 553, 673, 19267, 5223, 438, 1, 4360, 262, 584, 1595, 470, 954, 11, 780, 339, 6572, 340, 526, 198, 198, 1, 49174, 276, 340, 1701, 314, 373, 546, 284, 1061, 510, 428, 18437, 618, 314, 2982, 257, 2366, 9662, 290, 2497, 3619, 2241, 319, 262, 11387, 13, 198, 198, 1722, 339, 6204, 612, 11, 465, 2832, 287, 262, 16511, 286, 465, 11555, 303, 7821, 13209, 11, 262, 7888, 7586, 9813, 286, 4190, 7121, 736, 422, 465, 2330, 22645, 11, 465, 10904, 4252, 6236, 429, 25839, 9230, 808, 276, 416, 257, 8212, 326, 13663, 262, 9040, 286, 257, 2116, 12, 10414, 738, 285, 23968, 4891, 11, 314, 2936, 284, 644, 257, 4922, 339, 550, 262, 976, 3081, 355, 465, 5986, 438, 1169, 3081, 286, 2045, 1190, 4119, 81, 621, 339, 373, 13, 198, 198, 6653, 3656, 27846, 379, 683, 1207, 8344, 803, 306, 11, 475, 465, 2951, 21650, 1613, 607, 284, 262, 18560, 13, 198, 198, 1, 5246, 13, 8759, 2763, 2227, 284, 766, 340, 553, 673, 2540, 11, 355, 611, 2859, 3500, 5223, 13, 679, 28271, 465, 12450, 11, 991, 16755, 13, 198, 198, 1, 5812, 11, 8759, 2763, 1043, 502, 503, 890, 2084, 553, 339, 531, 15376, 26, 788, 11, 6427, 465, 3211, 832, 6164, 25, 366, 16773, 290, 766, 262, 1334, 286, 262, 2156, 526, 198, 198, 1544, 3751, 340, 284, 502, 351, 257, 1611, 286, 24354, 20154, 11293, 25, 262, 7837, 12, 9649, 11, 262, 5486, 12, 83, 29080, 11, 262, 6576, 12, 565, 418, 1039, 11, 262, 4057, 2655, 12, 8439, 274, 438, 439, 262, 3716, 7106, 6637, 286, 262, 45172, 338, 5928, 3773, 13, 843, 8797, 616, 4240, 3432, 262, 2938, 17547, 339, 531, 11, 9644, 503, 465, 7721, 257, 1310, 25, 366, 5297, 11, 314, 1107, 836, 470, 766, 703, 661, 6687, 284, 2107, 1231, 326, 526, 198, 198, 5779, 438, 270, 373, 655, 262, 886, 530, 1244, 423, 1674, 15898, 329, 683, 13, 5514, 339, 373, 11, 832, 340, 477, 290, 287, 15275, 286, 340, 477, 438, 292, 339, 550, 587, 832, 11, 290, 287, 15275, 286, 11, 465, 5986, 438, 568, 22665, 11, 523, 23332, 11, 523, 595, 18052, 11, 326, 530, 890, 276, 284, 3960, 503, 25, 366, 3856, 44455, 351, 534, 24638, 2474, 355, 1752, 530, 550, 890, 276, 284, 910, 25, 366, 3856, 44455, 351, 534, 670, 2474, 198, 198, 1537, 11, 351, 262, 3960, 319, 616, 11914, 11, 616, 13669, 6989, 281, 10059, 2198, 13, 198, 198, 1, 1212, 318, 616, 898, 49451, 553, 339, 531, 11, 3756, 502, 656, 257, 3223, 8631, 2119, 379, 262, 886, 286, 262, 781, 273, 312, 410, 12523, 13, 632, 373, 6616, 290, 7586, 290, 11620, 88, 25, 645, 366, 34435, 8172, 645, 865, 291, 12, 64, 12, 1671, 330, 11, 4844, 286, 262, 1633, 286, 24380, 329, 20728, 287, 257, 4286, 10273, 438, 29370, 477, 11, 645, 1551, 1051, 286, 1683, 1719, 587, 973, 355, 257, 8034, 13, 198, 198, 464, 1109, 3181, 1363, 284, 502, 262, 4112, 957, 1483, 286, 3619, 338, 2270, 351, 465, 1468, 1204, 13, 198, 198, 1, 3987, 470, 345, 1683, 45553, 903, 351, 7521, 597, 517, 1701, 314, 1965, 11, 991, 2045, 546, 329, 257, 12854, 286, 884, 3842, 13, 198, 198, 1, 12295, 553, 339, 531, 11589, 13, 198, 198, 1, 5574, 1660, 12, 49903, 438, 273, 2123, 10813, 1701, 198, 198, 6653, 6563, 2951, 6348, 5391, 11, 290, 465, 25839, 279, 3021, 257, 1310, 739, 511, 22665, 4252, 10899, 13, 198, 198, 1, 12295, 892, 286, 340, 11, 616, 13674, 5891, 438, 1092, 517, 621, 611, 314, 1549, 1239, 12615, 257, 14093, 526, 198, 198, 1870, 465, 8216, 1297, 502, 287, 257, 7644, 326, 339, 1239, 1807, 286, 1997, 2073, 13, 198, 198, 40, 3888, 1497, 11, 43045, 21100, 416, 616, 10059, 9412, 26, 290, 355, 314, 2900, 11, 616, 4151, 3214, 319, 257, 1402, 4286, 2029, 262, 24818, 417, 12, 12239, 438, 1169, 691, 2134, 7163, 262, 8631, 26210, 3425, 9417, 286, 262, 2119, 13, 198, 198, 1, 5812, 11, 416, 449, 659, 2474, 314, 531, 13, 198, 198, 1026, 373, 257, 17548, 286, 257, 50085, 438, 272, 1468, 10032, 50085, 11, 5055, 287, 262, 6290, 739, 257, 3355, 13, 198, 198, 1, 3886, 449, 659, 438, 64, 520, 5493, 2474, 314, 16896, 13, 198, 198, 1544, 373, 10574, 26, 475, 314, 2936, 683, 1969, 2157, 502, 11, 12704, 257, 1310, 2952, 13, 198, 198, 1, 2061, 257, 4240, 0, 14446, 351, 257, 8667, 3951, 438, 4360, 319, 45697, 19369, 13, 921, 9670, 28022, 11, 810, 750, 345, 651, 340, 1701, 198, 198, 1544, 9373, 6364, 25, 366, 27034, 13, 520, 5493, 2921, 340, 284, 502, 526, 198, 198, 1, 10910, 438, 40, 1422, 470, 760, 345, 772, 2993, 262, 520, 5493, 82, 13, 679, 373, 884, 281, 1167, 2588, 856, 607, 2781, 526, 198, 198, 1, 40, 1422, 470, 438, 83, 359, 706, 13, 764, 764, 764, 1375, 1908, 329, 502, 284, 7521, 683, 618, 339, 373, 2636, 526, 198, 198, 1, 2215, 339, 373, 2636, 30, 921, 1701, 198, 198, 40, 1276, 423, 1309, 257, 1310, 1165, 881, 40642, 972, 6654, 832, 616, 5975, 11, 329, 339, 9373, 351, 257, 1207, 8344, 803, 6487, 25, 366, 5297, 438, 7091, 338, 281, 12659, 2829, 1122, 11, 345, 760, 11, 9074, 13, 520, 5493, 13, 2332, 691, 2126, 373, 284, 423, 683, 1760, 416, 257, 38378, 34537, 438, 993, 11, 3595, 520, 5493, 0, 1375, 1807, 340, 262, 1654, 301, 835, 286, 46431, 465, 27951, 438, 1659, 10833, 340, 319, 257, 1308, 27461, 1171, 13, 843, 379, 262, 2589, 314, 373, 4808, 1169, 62, 38378, 34537, 526, 198, 198, 1, 10910, 11, 3595, 520, 5493, 438, 292, 345, 910, 13, 8920, 4808, 5562, 62, 465, 2106, 1701, 198, 198, 1, 2504, 373, 465, 2106, 13, 1375, 4762, 287, 683, 11, 26996, 798, 287, 683, 438, 273, 1807, 673, 750, 13, 887, 673, 3521, 470, 6842, 407, 284, 423, 477, 262, 8263, 12, 9649, 351, 607, 13, 1375, 3521, 470, 6842, 262, 1109, 326, 11, 319, 1401, 77, 3929, 1528, 11, 530, 714, 1464, 651, 1474, 1576, 284, 766, 465, 5986, 13, 23676, 2415, 0, 1375, 338, 655, 257, 24225, 39136, 278, 329, 584, 21441, 13, 520, 5493, 318, 262, 691, 2187, 314, 1683, 2993, 526, 198, 198, 1, 1639, 1683, 2993, 30, 887, 345, 655, 531, 438, 1, 198, 198, 38, 271, 10899, 550, 257, 11040, 8212, 287, 465, 2951, 13, 198, 198, 1, 5812, 11, 314, 2993, 683, 11, 290, 339, 2993, 502, 438, 8807, 340, 3022, 706, 339, 373, 2636, 526, 198, 198, 40, 5710, 616, 3809, 43045, 13, 366, 2215, 673, 1908, 329, 345, 1701, 198, 198, 1, 5297, 438, 37121, 1035, 27339, 284, 262, 21296, 13, 1375, 2227, 683, 29178, 3474, 438, 392, 416, 502, 2474, 198, 198, 1544, 13818, 757, 11, 290, 9617, 736, 465, 1182, 284, 804, 510, 379, 262, 17548, 286, 262, 50085, 13, 366, 1858, 547, 1528, 618, 314, 3521, 470, 804, 379, 326, 1517, 438, 24089, 77, 470, 1986, 340, 13, 887, 314, 4137, 3589, 284, 1234, 340, 994, 26, 290, 783, 340, 338, 30703, 502, 438, 66, 1522, 502, 13, 1320, 338, 262, 1738, 1521, 314, 836, 470, 45553, 903, 597, 517, 11, 616, 13674, 8759, 2763, 26, 393, 2138, 520, 5493, 2241, 318, 262, 1738, 526, 198, 198, 1890, 262, 717, 640, 616, 21696, 20136, 546, 616, 15185, 2900, 656, 257, 2726, 6227, 284, 1833, 683, 1365, 13, 198, 198, 1, 40, 4601, 345, 1549, 1560, 502, 703, 340, 3022, 553, 314, 531, 13, 198, 198, 1544, 6204, 2045, 510, 379, 262, 17548, 11, 290, 665, 24297, 1022, 465, 9353, 257, 17779, 339, 550, 11564, 284, 1657, 13, 24975, 339, 2900, 3812, 502, 13, 198, 198, 1, 40, 1549, 2138, 588, 284, 1560, 345, 438, 13893, 314, 1053, 1464, 9885, 345, 286, 2376, 26927, 616, 670, 526, 198, 198, 40, 925, 257, 1207, 8344, 803, 18342, 11, 543, 339, 2469, 265, 1572, 351, 257, 922, 12, 17047, 8167, 32545, 13, 198, 198, 1, 5812, 11, 314, 1422, 470, 1337, 257, 14787, 618, 314, 4762, 287, 3589, 438, 392, 783, 340, 338, 281, 2087, 9839, 1022, 514, 2474, 198, 198, 1544, 13818, 4622, 11, 1231, 35987, 11, 290, 7121, 530, 286, 262, 2769, 3211, 12, 49655, 2651, 13, 366, 1858, 25, 787, 3511, 6792, 438, 392, 994, 389, 262, 33204, 345, 588, 526, 198, 198, 1544, 4624, 606, 379, 616, 22662, 290, 3767, 284, 27776, 510, 290, 866, 262, 2119, 11, 12225, 783, 290, 788, 11061, 262, 4286, 13, 198, 198, 1, 2437, 340, 3022, 30, 314, 460, 1560, 345, 287, 1936, 2431, 438, 392, 340, 1422, 470, 1011, 881, 2392, 284, 1645, 13, 764, 764, 764, 314, 460, 3505, 783, 703, 6655, 290, 10607, 314, 373, 618, 314, 1392, 9074, 13, 520, 5493, 338, 3465, 13, 3226, 1781, 11, 2769, 866, 11, 314, 550, 1464, 4808, 31985, 62, 612, 373, 645, 530, 588, 683, 438, 8807, 314, 550, 3750, 351, 262, 4269, 11, 22211, 262, 6678, 40315, 10455, 546, 683, 11, 10597, 314, 2063, 1392, 284, 892, 339, 373, 257, 5287, 11, 530, 286, 262, 1611, 326, 389, 1364, 2157, 13, 2750, 449, 659, 11, 290, 339, 4808, 9776, 62, 1364, 2157, 438, 13893, 339, 550, 1282, 284, 2652, 0, 383, 1334, 286, 514, 550, 284, 1309, 6731, 307, 17676, 1863, 393, 467, 739, 11, 475, 339, 373, 1029, 2029, 262, 1459, 438, 261, 45697, 19369, 11, 355, 345, 910, 13, 198, 198, 1, 5779, 11, 314, 1816, 572, 284, 262, 2156, 287, 616, 749, 34372, 10038, 438, 34330, 3888, 11, 4453, 20927, 502, 11, 379, 262, 3108, 418, 286, 3595, 520, 5493, 338, 3451, 286, 5287, 852, 37492, 416, 262, 13476, 286, 616, 12036, 683, 0, 3226, 1781, 314, 4001, 284, 466, 262, 4286, 329, 2147, 438, 40, 1297, 9074, 13, 520, 5493, 523, 618, 673, 2540, 284, 336, 321, 647, 1223, 546, 607, 8098, 13, 314, 3505, 1972, 572, 257, 40426, 10956, 9546, 546, 262, 15393, 852, 4808, 3810, 62, 438, 1219, 11, 314, 373, 19716, 306, 11, 616, 13674, 8759, 2763, 0, 314, 373, 24380, 284, 3589, 588, 530, 286, 616, 898, 1650, 1010, 13, 198, 198, 1, 6423, 314, 373, 2077, 510, 290, 1364, 3436, 351, 683, 13, 314, 550, 1908, 477, 616, 20348, 287, 5963, 11, 290, 314, 550, 691, 284, 900, 510, 262, 1396, 417, 290, 651, 284, 670, 13, 679, 550, 587, 2636, 691, 8208, 12, 14337, 2250, 11, 290, 339, 3724, 6451, 11, 286, 2612, 4369, 11, 523, 326, 612, 550, 587, 645, 15223, 670, 286, 8166, 438, 14363, 1986, 373, 1598, 290, 36519, 13, 314, 550, 1138, 683, 1752, 393, 5403, 11, 812, 878, 11, 290, 1807, 683, 32081, 290, 44852, 88, 13, 2735, 314, 2497, 326, 339, 373, 21840, 13, 198, 198, 1, 40, 373, 9675, 379, 717, 11, 351, 257, 6974, 19713, 14676, 25, 9675, 284, 423, 616, 1021, 319, 884, 257, 705, 32796, 2637, 3244, 465, 6283, 1204, 12, 46965, 9449, 2540, 284, 2689, 502, 24506, 306, 438, 292, 314, 10226, 262, 1182, 287, 314, 2936, 355, 611, 339, 547, 4964, 502, 466, 340, 13, 383, 18098, 373, 3940, 416, 262, 1807, 25, 611, 339, 4808, 22474, 62, 4964, 502, 11, 644, 561, 339, 910, 284, 616, 835, 286, 1762, 30, 2011, 29483, 2540, 284, 467, 257, 1310, 4295, 438, 40, 2936, 10927, 290, 8627, 13, 198, 198, 1, 7454, 11, 618, 314, 3114, 510, 11, 314, 3947, 284, 766, 257, 8212, 2157, 465, 1969, 12768, 680, 21213, 438, 292, 611, 339, 550, 262, 3200, 11, 290, 547, 28297, 2241, 416, 4769, 340, 736, 422, 502, 13, 1320, 41851, 515, 502, 991, 517, 13, 383, 3200, 30, 4162, 11, 314, 550, 257, 3200, 2861, 8208, 286, 465, 0, 314, 37901, 379, 262, 21978, 44896, 11, 290, 3088, 617, 286, 616, 49025, 5330, 15910, 13, 887, 484, 4054, 502, 11, 484, 1067, 11137, 13, 314, 2497, 326, 339, 2492, 470, 4964, 262, 905, 88, 10340, 438, 40, 3521, 470, 11786, 465, 3241, 26, 339, 655, 4030, 465, 2951, 319, 262, 1327, 22674, 1022, 13, 5845, 547, 262, 3392, 314, 550, 1464, 427, 343, 9091, 11, 393, 5017, 510, 351, 617, 9105, 7521, 13, 843, 703, 339, 2497, 832, 616, 7363, 0, 198, 198, 1, 40, 3114, 510, 757, 11, 290, 4978, 6504, 286, 326, 17548, 286, 262, 50085, 10938, 319, 262, 3355, 1474, 465, 3996, 13, 2399, 3656, 1297, 502, 20875, 340, 373, 262, 938, 1517, 339, 550, 1760, 438, 3137, 257, 3465, 2077, 351, 257, 17275, 1021, 11, 618, 339, 373, 866, 287, 6245, 684, 10695, 20222, 422, 257, 2180, 2612, 1368, 13, 2329, 257, 3465, 0, 887, 340, 4952, 465, 2187, 2106, 13, 1318, 389, 812, 286, 5827, 40987, 913, 30802, 287, 790, 1627, 13, 317, 582, 508, 550, 1509, 388, 351, 262, 1459, 714, 1239, 423, 4499, 326, 18680, 510, 12, 5532, 14000, 13, 764, 764, 764, 198, 198, 1, 40, 2900, 736, 284, 616, 670, 11, 290, 1816, 319, 39136, 278, 290, 285, 4185, 1359, 26, 788, 314, 3114, 379, 262, 50085, 757, 13, 314, 2497, 326, 11, 618, 520, 5493, 8104, 287, 262, 717, 14000, 11, 339, 2993, 655, 644, 262, 886, 561, 307, 13, 679, 550, 17273, 465, 2426, 11, 19233, 340, 11, 11027, 515, 340, 13, 1649, 550, 314, 1760, 326, 351, 597, 286, 616, 1243, 30, 1119, 8020, 470, 587, 4642, 286, 502, 438, 40, 550, 655, 8197, 606, 13, 764, 764, 764, 198, 198, 1, 39, 648, 340, 11, 8759, 2763, 11, 351, 326, 1986, 4964, 502, 314, 3521, 470, 466, 1194, 14000, 13, 383, 8631, 3872, 373, 11, 314, 1422, 470, 760, 810, 284, 1234, 340, 438, 62, 40, 550, 1239, 1900, 44807, 5514, 11, 351, 616, 1650, 1010, 290, 616, 1171, 11, 257, 905, 88, 22870, 286, 9568, 5017, 510, 262, 1109, 438, 40, 655, 9617, 7521, 656, 511, 6698, 13, 764, 764, 764, 3894, 11, 7521, 373, 262, 530, 7090, 883, 2636, 2951, 714, 766, 832, 438, 3826, 3892, 284, 262, 2006, 20212, 19369, 14638, 13, 2094, 470, 345, 760, 703, 11, 287, 3375, 257, 3215, 3303, 11, 772, 6562, 1473, 11, 530, 1139, 2063, 262, 640, 407, 644, 530, 3382, 284, 475, 644, 530, 460, 30, 3894, 438, 5562, 373, 262, 835, 314, 13055, 26, 290, 355, 339, 3830, 612, 290, 7342, 502, 11, 262, 1517, 484, 1444, 616, 705, 23873, 2350, 6, 14707, 588, 257, 2156, 286, 4116, 13, 679, 1422, 470, 10505, 263, 11, 345, 1833, 11, 3595, 520, 5493, 438, 258, 655, 3830, 612, 12703, 4964, 11, 290, 319, 465, 11914, 11, 832, 262, 12768, 21213, 11, 314, 3947, 284, 3285, 262, 1808, 25, 705, 8491, 345, 1654, 345, 760, 810, 345, 821, 2406, 503, 8348, 198, 198, 1, 1532, 314, 714, 423, 13055, 326, 1986, 11, 351, 326, 1808, 319, 340, 11, 314, 815, 423, 1760, 257, 1049, 1517, 13, 383, 1306, 6000, 1517, 373, 284, 766, 326, 314, 3521, 470, 438, 392, 326, 11542, 373, 1813, 502, 13, 887, 11, 11752, 11, 379, 326, 5664, 11, 8759, 2763, 11, 373, 612, 1997, 319, 4534, 314, 3636, 470, 423, 1813, 284, 423, 520, 5493, 6776, 878, 502, 11, 290, 284, 3285, 683, 910, 25, 705, 1026, 338, 407, 1165, 2739, 438, 40, 1183, 905, 345, 703, 30960, 198, 198, 1, 1026, 4808, 9776, 62, 1165, 2739, 438, 270, 561, 423, 587, 11, 772, 611, 339, 1549, 587, 6776, 13, 314, 11856, 510, 616, 20348, 11, 290, 1816, 866, 290, 1297, 9074, 13, 520, 5493, 13, 3226, 1781, 314, 1422, 470, 1560, 607, 4808, 5562, 62, 438, 270, 561, 423, 587, 8312, 284, 607, 13, 314, 2391, 531, 314, 3521, 470, 7521, 683, 11, 326, 314, 373, 1165, 3888, 13, 1375, 2138, 8288, 262, 2126, 438, 7091, 338, 523, 14348, 0, 632, 373, 326, 326, 925, 607, 1577, 502, 262, 50085, 13, 887, 673, 373, 22121, 9247, 379, 407, 1972, 262, 18560, 438, 7091, 750, 523, 765, 683, 705, 28060, 6, 416, 617, 530, 905, 88, 0, 1629, 717, 314, 373, 7787, 673, 3636, 470, 1309, 502, 572, 438, 392, 379, 616, 266, 896, 6, 886, 314, 5220, 41379, 293, 13, 3363, 11, 340, 373, 314, 508, 2067, 41379, 293, 25, 314, 1297, 9074, 13, 520, 5493, 339, 373, 262, 705, 4976, 6, 582, 11, 290, 673, 1297, 8276, 2073, 11, 290, 523, 340, 1392, 284, 307, 2081, 13, 764, 764, 764, 843, 339, 13055, 520, 5493, 1231, 1592, 2259, 26, 290, 673, 9174, 262, 4286, 1871, 607, 5229, 338, 1243, 13, 764, 764, 22135, 198, 198, 1544, 45111, 2241, 866, 287, 262, 3211, 12, 16337, 1474, 6164, 11, 8104, 736, 465, 1182, 11, 290, 47425, 278, 465, 5101, 11061, 340, 11, 3114, 510, 379, 262, 4286, 2029, 262, 18205, 1681, 12, 12239, 13, 198, 198, 1, 40, 588, 284, 14996, 326, 520, 5493, 2241, 561, 423, 1813, 340, 284, 502, 11, 611, 339, 1549, 587, 1498, 284, 910, 644, 339, 1807, 326, 1110, 526, 198, 198, 1870, 11, 287, 3280, 284, 257, 1808, 314, 1234, 2063, 12, 1326, 3147, 1146, 438, 1, 44140, 757, 1701, 339, 30050, 503, 13, 366, 2215, 262, 530, 1517, 326, 6774, 502, 6609, 1474, 683, 318, 326, 314, 2993, 1576, 284, 2666, 572, 1701, 198, 198, 1544, 6204, 510, 290, 8104, 465, 1021, 319, 616, 8163, 351, 257, 6487, 13, 366, 10049, 262, 21296, 286, 340, 318, 326, 314, 4808, 321, 62, 991, 12036, 438, 20777, 41379, 293, 338, 1804, 340, 329, 502, 0, 383, 520, 5493, 82, 1302, 3436, 11, 290, 1645, 1752, 438, 4360, 612, 338, 645, 42393, 803, 674, 1611, 286, 1242, 526]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One of the easiest and most intuitive ways to create the input-targer pairs of the nextword prediction task is to create two variables, x and y. Where x contains the input tokens and y contains the target, which are the inputs shifted by 1:**"
      ],
      "metadata": {
        "id": "U16Z8FTl1ctC"
      },
      "id": "U16Z8FTl1ctC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The context size determines how many tokens are included in the input"
      ],
      "metadata": {
        "id": "W7gj2F9l3J4g"
      },
      "id": "W7gj2F9l3J4g"
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 10 #length of the input\n",
        "# The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
        "# to predict the next word in the sequence.\n",
        "# The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 token [2, 3. 4. 5]\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")\n",
        "# Decode tokens to readable text\n",
        "decoded_x = tokenizer.decode(x)\n",
        "decoded_y = tokenizer.decode(y)\n",
        "\n",
        "print(f\"Decoded x: {decoded_x}\")\n",
        "print(f\"Decoded y: {decoded_y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klr4FYUD1JSD",
        "outputId": "d0fdc7db-ed09-41ee-f44c-8e1f204c8d3e"
      },
      "id": "klr4FYUD1JSD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686]\n",
            "y:      [4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976]\n",
            "Decoded x:  and established himself in a villa on the Riv\n",
            "Decoded y:  established himself in a villa on the Riviera\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the inputs along with the taget, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:"
      ],
      "metadata": {
        "id": "HIhMKBMe5L40"
      },
      "id": "HIhMKBMe5L40"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNHopxqW5CEJ",
        "outputId": "e1f06bfd-d2d9-4d0d-9525-3729717b67ad"
      },
      "id": "sNHopxqW5CEJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n",
            "[290, 4920, 2241, 287, 257] ----> 4489\n",
            "[290, 4920, 2241, 287, 257, 4489] ----> 64\n",
            "[290, 4920, 2241, 287, 257, 4489, 64] ----> 319\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319] ----> 262\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262] ----> 34686\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686] ----> 41976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict."
      ],
      "metadata": {
        "id": "ggzGKlip-siV"
      },
      "id": "ggzGKlip-siV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For illustration purposes, let's repeat the previous code but convert the token IDs into text."
      ],
      "metadata": {
        "id": "HSdj54g8_n7N"
      },
      "id": "HSdj54g8_n7N"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzmcKoWd-MCs",
        "outputId": "66302fb8-bbc7-429f-a492-efec51ee645b"
      },
      "id": "TzmcKoWd-MCs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n",
            " and established himself in a ---->  vill\n",
            " and established himself in a vill ----> a\n",
            " and established himself in a villa ---->  on\n",
            " and established himself in a villa on ---->  the\n",
            " and established himself in a villa on the ---->  Riv\n",
            " and established himself in a villa on the Riv ----> iera\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now created the input-target pairs what we can turn into use fot the LLM training in upcoming chapters."
      ],
      "metadata": {
        "id": "LsssHr55CgxB"
      },
      "id": "LsssHr55CgxB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays."
      ],
      "metadata": {
        "id": "UbogoHcJCsVL"
      },
      "id": "UbogoHcJCsVL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict,"
      ],
      "metadata": {
        "id": "TC-nn1WHCyvh"
      },
      "id": "TC-nn1WHCyvh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPLEMENTING A DATA LOADER**"
      ],
      "metadata": {
        "id": "2uxEBLDhDhS_"
      },
      "id": "2uxEBLDhDhS_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and DataLoader classes."
      ],
      "metadata": {
        "id": "t0AAaCWIDtsu"
      },
      "id": "t0AAaCWIDtsu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Tokenize the entire text\n",
        "\n",
        "**Step 2:** Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "\n",
        "**Step 3:** Return the total number of rows in the dataset\n",
        "\n",
        "**Step 4:** Return a single row from the dataset"
      ],
      "metadata": {
        "id": "WswoKktBGFKp"
      },
      "id": "WswoKktBGFKp"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "L66La1GrAk_O"
      },
      "id": "L66La1GrAk_O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "Dzqf5ZjOJPZJ"
      },
      "id": "Dzqf5ZjOJPZJ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACcK8vVjX-zZ"
      },
      "id": "ACcK8vVjX-zZ",
      "execution_count": null,
      "outputs": []
    }
  ]
}